{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e6ad75-ffc7-4dbc-b2f1-22ba7223287e",
   "metadata": {},
   "source": [
    "The bounding box, creates a crop of the image and takes aways the unimportant stuff (branches, leaves and ...) andjust leaves the birds.\n",
    "\n",
    "The cropped image then goes through the transform pipeline:\n",
    "- Resize ==> To make the images uniform!\n",
    "- Totensor ==> To convert the images into a pyTorch Tensor. meanig that you go from [H,W,C] to [C,H,W], C standig for the channel that has been normalized to be between 0 and 1 (critical for the calculations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeceba2-5de7-4ad9-a2e6-20378e708342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steps 1 and 2\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# --- Load all metadata (we'll split it ourselves) ---\n",
    "# Use the correct path with forward slashes or as a raw string\n",
    "CUB_ROOT_DIR = r'D:\\Images\\CUB_200_2011_reduced\\CUB_200_2011'\n",
    "IMAGE_DIR = os.path.join(CUB_ROOT_DIR, 'images')\n",
    "\n",
    "# Read metadata files\n",
    "images_df = pd.read_csv(os.path.join(CUB_ROOT_DIR, 'images.txt'), sep=' ', names=['img_id', 'filepath'])\n",
    "labels_df = pd.read_csv(os.path.join(CUB_ROOT_DIR, 'image_class_labels.txt'), sep=' ', names=['img_id', 'class_id'])\n",
    "split_df = pd.read_csv(os.path.join(CUB_ROOT_DIR, 'train_test_split.txt'), sep=' ', names=['img_id', 'is_train'])\n",
    "classes_df = pd.read_csv(os.path.join(CUB_ROOT_DIR, 'classes.txt'), sep=' ', names=['class_id', 'class_name'])\n",
    "\n",
    "# Merge, 0-index the class_id, and filter for first 10 classes\n",
    "all_data_df = images_df.merge(labels_df, on='img_id').merge(split_df, on='img_id')\n",
    "all_data_df['class_id'] = all_data_df['class_id'] - 1\n",
    "all_data_df = all_data_df[all_data_df['class_id'] < 10]\n",
    "\n",
    "# --- 1. Show an image for each class ---\n",
    "print(\"--- Task 1: Sample Image Per Class ---\")\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    # Find the first image for this class\n",
    "    sample_row = all_data_df[all_data_df['class_id'] == i].iloc[0]\n",
    "    img_path = os.path.join(IMAGE_DIR, sample_row['filepath'])\n",
    "    class_name = classes_df[classes_df['class_id'] == (i + 1)]['class_name'].values[0].split('.')[-1]\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Class {i+1}: {class_name}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Sample Image for Each of the 10 Classes\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 2. Show statistics of groups for training and validation ---\n",
    "print(\"\\n--- Task 2: Train/Validation Statistics per Class ---\")\n",
    "train_stats = all_data_df[all_data_df['is_train'] == 1]['class_id'].value_counts().sort_index()\n",
    "test_stats = all_data_df[all_data_df['is_train'] == 0]['class_id'].value_counts().sort_index()\n",
    "\n",
    "stats_df = pd.DataFrame({'Train': train_stats, 'Test': test_stats})\n",
    "\n",
    "# --- NEW LINES TO CHANGE THE INDEX ---\n",
    "stats_df.index = stats_df.index + 1  # Convert 0-9 index to 1-10\n",
    "print(\"Statistics (Classes 1-10):\")\n",
    "# --- END NEW LINES ---\n",
    "\n",
    "print(stats_df)\n",
    "\n",
    "# Plotting the statistics\n",
    "stats_df.plot(kind='bar', figsize=(12, 6), rot=0)\n",
    "plt.title(\"Number of Images per Class (Train vs. Test)\")\n",
    "# --- THIS LINE IS CHANGED ---\n",
    "plt.xlabel(\"Class ID (1-10)\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d44405-df0e-4698-9a99-9674dd09691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Attribute Data ---\n",
    "\n",
    "# Get the base directory (one level up from CUB_ROOT_DIR)\n",
    "# e.g., 'D:\\...\\CUB_200_2011_reduced'\n",
    "BASE_DIR = os.path.dirname(CUB_ROOT_DIR)\n",
    "\n",
    "# 1. Load attribute names from the base directory\n",
    "try:\n",
    "    attributes_df = pd.read_csv(\n",
    "        os.path.join(BASE_DIR, 'attributes.txt'),  # <-- FIXED PATH\n",
    "        sep=' ', \n",
    "        names=['attr_id', 'attr_name']\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    # Handle the case where the user's CUB_ROOT_DIR is the *actual* root\n",
    "    attributes_df = pd.read_csv(\n",
    "        os.path.join(CUB_ROOT_DIR, '..', 'attributes.txt'), # Try relative path\n",
    "        sep=' ', \n",
    "        names=['attr_id', 'attr_name']\n",
    "    )\n",
    "\n",
    "\n",
    "# 2. Load which images have which attributes (this path was correct)\n",
    "img_attr_df = pd.read_csv(\n",
    "    os.path.join(CUB_ROOT_DIR, 'attributes', 'image_attribute_labels.txt'),\n",
    "    sep=' ', \n",
    "    names=['img_id', 'attr_id', 'is_present', 'certainty_id', 'time']\n",
    ")\n",
    "\n",
    "# --- Pick one attribute to analyze, e.g., \"in water\" ---\n",
    "# Let's find the attribute for \"in water\"\n",
    "try:\n",
    "    water_attr = attributes_df[attributes_df['attr_name'].str.contains('water')].iloc[0]\n",
    "    water_attr_id = water_attr['attr_id']\n",
    "    print(f\"Found attribute: {water_attr['attr_name']} (ID: {water_attr_id})\\n\")\n",
    "\n",
    "    # Get all image_ids from our 10-class set\n",
    "    our_img_ids = set(all_data_df['img_id'])\n",
    "\n",
    "    # Filter the attribute labels for just our images and this attribute\n",
    "    water_labels = img_attr_df[\n",
    "        (img_attr_df['img_id'].isin(our_img_ids)) &\n",
    "        (img_attr_df['attr_id'] == water_attr_id) &\n",
    "        (img_attr_df['is_present'] == 1) # 1 = True, 0 = False\n",
    "    ]\n",
    "\n",
    "    # Merge with our main data to see class distribution\n",
    "    water_data = all_data_df.merge(water_labels, on='img_id', how='left')\n",
    "    water_data['is_in_water'] = water_data['is_present'].fillna(0).astype(int)\n",
    "\n",
    "    # --- 1. Show an image for each group (e.g., in_water vs. not_in_water) ---\n",
    "    print(f\"--- Task 1 (Advanced): Sample Images for Attribute '{water_attr['attr_name']}' ---\")\n",
    "    \n",
    "    # Check if we have samples for both groups\n",
    "    if 1 in water_data['is_in_water'].values and 0 in water_data['is_in_water'].values:\n",
    "        img_in_water = water_data[water_data['is_in_water'] == 1].iloc[0]\n",
    "        img_not_in_water = water_data[water_data['is_in_water'] == 0].iloc[0]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        img_path_1 = os.path.join(IMAGE_DIR, img_in_water['filepath'])\n",
    "        axes[0].imshow(Image.open(img_path_1))\n",
    "        axes[0].set_title(\"Group: 'is_in_water' = TRUE\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        img_path_2 = os.path.join(IMAGE_DIR, img_not_in_water['filepath'])\n",
    "        axes[1].imshow(Image.open(img_path_2))\n",
    "        axes[1].set_title(\"Group: 'is_in_water' = FALSE\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Could not find images for both 'in water' and 'not in water' groups.\")\n",
    "\n",
    "\n",
    "    # --- 2. Show statistics of groups for training and validation ---\n",
    "    print(f\"\\n--- Task 2 (Advanced): Statistics for Attribute '{water_attr['attr_name']}' ---\")\n",
    "    train_water_stats = water_data[water_data['is_train'] == 1]['is_in_water'].value_counts().reindex([0, 1], fill_value=0)\n",
    "    test_water_stats = water_data[water_data['is_train'] == 0]['is_in_water'].value_counts().reindex([0, 1], fill_value=0)\n",
    "    \n",
    "    stats_df_attr = pd.DataFrame({'Train': train_water_stats, 'Test': test_water_stats})\n",
    "    stats_df_attr.index = ['Not in Water', 'In Water']\n",
    "    print(stats_df_attr)\n",
    "    \n",
    "    stats_df_attr.plot(kind='bar', figsize=(8, 5), rot=0)\n",
    "    plt.title(f\"Distribution of Attribute: '{water_attr['attr_name']}'\")\n",
    "    plt.ylabel(\"Number of Images\")\n",
    "    plt.show()\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Could not find an attribute containing 'water' in your dataset.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find 'attributes.txt' at the expected location.\")\n",
    "    print(f\"Looked in: {os.path.join(BASE_DIR, 'attributes.txt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ea309-cd00-4283-be56-f75a277c6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da3434-839e-4d5f-bbb8-913187f74ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A network that uses a pretrained ResNet18 backbone\n",
    "    to extract embeddings from images.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim (int): The dimension of the output embedding.\n",
    "        \"\"\"\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        \n",
    "        # 1. Load the pretrained ResNet18\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # 2. Get the number of features from the layer *before* the classifier\n",
    "        # In ResNet18, this layer is named 'fc'\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # 3. Replace the final classifier layer ('fc') with our new embedding layer\n",
    "        # We just use a simple linear layer.\n",
    "        self.backbone.fc = nn.Linear(num_features, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input batch of images [B, C, H, W]\n",
    "        Returns:\n",
    "            torch.Tensor: Embeddings [B, embedding_dim]\n",
    "        \"\"\"\n",
    "        # 1. Pass image through the modified ResNet\n",
    "        embeddings = self.backbone(x)\n",
    "        \n",
    "        # 2. Normalize the embeddings (L2 normalization)\n",
    "        # This is the \"Normalization (Optional)\" step.\n",
    "        # It makes all embedding vectors have a length of 1,\n",
    "        # which is standard practice for triplet loss.\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Also, make sure the 'device' variable is defined\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2b466-d86e-4093-8f19-56ec963d6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm # Make sure tqdm is imported\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- 1. Define the Evaluation Dataset Class ---\n",
    "# This class returns (image, label) pairs\n",
    "class CUBEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that returns just one image and its label.\n",
    "    Used for generating embeddings for the entire test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='test', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(self.root_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        # Load metadata (same as before)\n",
    "        images_df = pd.read_csv(os.path.join(self.root_dir, 'images.txt'), sep=' ', names=['img_id', 'filepath'])\n",
    "        labels_df = pd.read_csv(os.path.join(self.root_dir, 'image_class_labels.txt'), sep=' ', names=['img_id', 'class_id'])\n",
    "        split_df = pd.read_csv(os.path.join(self.root_dir, 'train_test_split.txt'), sep=' ', names=['img_id', 'is_train'])\n",
    "        \n",
    "        data_df = images_df.merge(labels_df, on='img_id').merge(split_df, on='img_id')\n",
    "        data_df['class_id'] = data_df['class_id'] - 1 # 0-indexed\n",
    "        \n",
    "        # Filter for first 10 classes. this should be removed for the real datase\n",
    "        data_df = data_df[data_df['class_id'] < 10].reset_index(drop=True)\n",
    "        \n",
    "        # Filter by split\n",
    "        target_split = 1 if self.split == 'train' else 0\n",
    "        self.data_df = data_df[data_df['is_train'] == target_split].reset_index(drop=True)\n",
    "        \n",
    "        self.data_list = list(zip(self.data_df['filepath'], self.data_df['class_id']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def _load_image(self, filepath):\n",
    "        full_path = os.path.join(self.image_dir, filepath)\n",
    "        img = Image.open(full_path).convert('RGB')\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the image and its label\n",
    "        img_path, label = self.data_list[index]\n",
    "        img = self._load_image(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, label\n",
    "\n",
    "# --- 2. Define the 'get_all_embeddings' function ---\n",
    "def get_all_embeddings(model, loader, device):\n",
    "    model.eval() # Set model to eval mode\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Getting Embeddings\"):\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            \n",
    "    # Concatenate all batches\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    return all_embeddings, all_labels\n",
    "\n",
    "# --- 3. Define the 'eval_loader' and 'data_transform' ---\n",
    "# (This may also be undefined if you restarted the notebook)\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eval_dataset = CUBEmbeddingDataset(\n",
    "    root_dir=CUB_ROOT_DIR,\n",
    "    split='test',\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False, # No shuffling!\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf277edb-bf56-4cff-99b1-f4e49d7ec3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import transforms\n",
    "import umap # Needs: pip install umap-learn\n",
    "\n",
    "# --- 1. Get \"baseline\" embeddings ---\n",
    "# We use a FRESH, untrained model to get the baseline embeddings.\n",
    "# This shows us the feature space before our triplet loss training.\n",
    "print(\"--- Task 3: Baseline Visualization (Before Training) ---\")\n",
    "print(\"Initializing a fresh, pre-trained ResNet18...\")\n",
    "\n",
    "# Make sure you have your EmbeddingNet class defined from the previous steps\n",
    "fresh_model = EmbeddingNet(embedding_dim=128) \n",
    "fresh_model.to(device)\n",
    "fresh_model.eval() # Set to evaluation mode\n",
    "\n",
    "# We can re-use the `get_all_embeddings` function and `eval_loader`\n",
    "# to get embeddings from this new \"fresh\" model.\n",
    "print(\"Generating baseline embeddings for the test set...\")\n",
    "base_embeddings, base_labels = get_all_embeddings(fresh_model, eval_loader, device)\n",
    "\n",
    "print(f\"Got {base_embeddings.shape[0]} baseline embeddings.\")\n",
    "\n",
    "\n",
    "# --- 2. Run t-SNE ---\n",
    "print(\"Running t-SNE... (this may take a moment)\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n",
    "tsne_embeddings = tsne.fit_transform(base_embeddings)\n",
    "print(\"t-SNE finished.\")\n",
    "\n",
    "# --- 3. Run UMAP ---\n",
    "print(\"Running UMAP...\")\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_embeddings = umap_reducer.fit_transform(base_embeddings)\n",
    "print(\"UMAP finished.\")\n",
    "\n",
    "# --- 4. Plot both ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# t-SNE Plot\n",
    "axes[0].set_title('t-SNE of Baseline Embeddings (Before Training)')\n",
    "for i in range(10): # We have 10 classes\n",
    "    indices = (base_labels == i)\n",
    "    axes[0].scatter(\n",
    "        tsne_embeddings[indices, 0], \n",
    "        tsne_embeddings[indices, 1], \n",
    "        alpha=0.6,\n",
    "        label=f'Class {i+1}'\n",
    "    )\n",
    "axes[0].set_xlabel('t-SNE Component 1')\n",
    "axes[0].set_ylabel('t-SNE Component 2')\n",
    "axes[0].legend()\n",
    "\n",
    "# UMAP Plot\n",
    "axes[1].set_title('UMAP of Baseline Embeddings (Before Training)')\n",
    "for i in range(10): # We have 10 classes\n",
    "    indices = (base_labels == i)\n",
    "    axes[1].scatter(\n",
    "        umap_embeddings[indices, 0], \n",
    "        umap_embeddings[indices, 1], \n",
    "        alpha=0.6,\n",
    "        label=f'Class {i+1}'\n",
    "    )\n",
    "axes[1].set_xlabel('UMAP Component 1')\n",
    "axes[1].set_ylabel('UMAP Component 2')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle(\"Visualization of Raw Image Features (from pre-trained ResNet18)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b96c48-58b3-48db-9a56-6529e1cdaff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class TripletCUBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading CUB-200 for triplet loss.\n",
    "    It reads the metadata files to create a list of all images,\n",
    "    and then organizes them by class to enable efficient triplet sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Path to the CUB_200_2011 directory.\n",
    "            split (string): 'train' or 'test' to load the respective split.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(self.root_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        # Load metadata\n",
    "        self._load_metadata()\n",
    "        \n",
    "    def _load_metadata(self):\n",
    "        # Read images.txt: <image_id> <filepath>\n",
    "        images_df = pd.read_csv(\n",
    "            os.path.join(self.root_dir, 'images.txt'),\n",
    "            sep=' ',\n",
    "            names=['img_id', 'filepath']\n",
    "        )\n",
    "        \n",
    "        # Read image_class_labels.txt: <image_id> <class_id>\n",
    "        labels_df = pd.read_csv(\n",
    "            os.path.join(self.root_dir, 'image_class_labels.txt'),\n",
    "            sep=' ',\n",
    "            names=['img_id', 'class_id']\n",
    "        )\n",
    "        \n",
    "        # Read train_test_split.txt: <image_id> <is_train>\n",
    "        split_df = pd.read_csv(\n",
    "            os.path.join(self.root_dir, 'train_test_split.txt'),\n",
    "            sep=' ',\n",
    "            names=['img_id', 'is_train']\n",
    "        )\n",
    "        \n",
    "        # Merge dataframes\n",
    "        data_df = images_df.merge(labels_df, on='img_id').merge(split_df, on='img_id')\n",
    "        \n",
    "        # PyTorch class IDs are typically 0-indexed\n",
    "        # CUB-200 class IDs are 1-indexed (1-200)\n",
    "        data_df['class_id'] = data_df['class_id'] - 1\n",
    "        \n",
    "        # Filter by split (1 for train, 0 for test)\n",
    "        target_split = 1 if self.split == 'train' else 0\n",
    "        self.data_df = data_df[data_df['is_train'] == target_split].reset_index(drop=True)\n",
    "\n",
    "        # --- This is the key part for triplet sampling ---\n",
    "        # 1. Create a list of all data points (filepath, class_id)\n",
    "        self.data_list = list(zip(self.data_df['filepath'], self.data_df['class_id']))\n",
    "        \n",
    "        # 2. Create a dictionary mapping class_id -> [list of indices in self.data_list]\n",
    "        self.class_to_indices = {}\n",
    "        for idx, (_, class_id) in enumerate(self.data_list):\n",
    "            if class_id not in self.class_to_indices:\n",
    "                self.class_to_indices[class_id] = []\n",
    "            self.class_to_indices[class_id].append(idx)\n",
    "            \n",
    "        # 3. Store a list of all unique class IDs\n",
    "        self.classes = list(self.class_to_indices.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def _load_image(self, filepath):\n",
    "        \"\"\"Helper to load an image from its relative path.\"\"\"\n",
    "        full_path = os.path.join(self.image_dir, filepath)\n",
    "        img = Image.open(full_path).convert('RGB')\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates one triplet (Anchor, Positive, Negative).\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Get the ANCHOR image\n",
    "        anchor_path, anchor_class = self.data_list[index]\n",
    "        anchor_img = self._load_image(anchor_path)\n",
    "        \n",
    "        # 2. Get a POSITIVE image (same class, different image) \n",
    "        positive_indices = self.class_to_indices[anchor_class]\n",
    "        \n",
    "        # Ensure we don't pick the same image as the anchor\n",
    "        positive_index = index\n",
    "        while positive_index == index and len(positive_indices) > 1:\n",
    "            positive_index = random.choice(positive_indices)\n",
    "            \n",
    "        positive_path, _ = self.data_list[positive_index]\n",
    "        positive_img = self._load_image(positive_path)\n",
    "        \n",
    "        # 3. Get a NEGATIVE image (different class) \n",
    "        negative_class = anchor_class\n",
    "        while negative_class == anchor_class:\n",
    "            negative_class = random.choice(self.classes)\n",
    "            \n",
    "        negative_indices = self.class_to_indices[negative_class]\n",
    "        negative_index = random.choice(negative_indices)\n",
    "        \n",
    "        negative_path, _ = self.data_list[negative_index]\n",
    "        negative_img = self._load_image(negative_path)\n",
    "\n",
    "        # 4. Apply transforms\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            positive_img = self.transform(positive_img)\n",
    "            negative_img = self.transform(negative_img)\n",
    "            \n",
    "        return anchor_img, positive_img, negative_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97dcc47-c527-4e52-8c62-761d982e0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "# Define transforms\n",
    "# These are standard transforms for a pretrained model like ResNet\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Your CUB_200_2011 directory path\n",
    "CUB_ROOT_DIR = 'D:/images/CUB_200_2011_reduced/CUB_200_2011' \n",
    "\n",
    "# 1. Create the Dataset\n",
    "train_dataset = TripletCUBDataset(\n",
    "    root_dir=CUB_ROOT_DIR,\n",
    "    split='train',\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "test_dataset = TripletCUBDataset(\n",
    "    root_dir=CUB_ROOT_DIR,\n",
    "    split='test',\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "# 2. Create the DataLoader\n",
    "# You can tune batch_size and num_workers\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,  # Shuffle is important for good training\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False, # No need to shuffle test data\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# 3. Test the loader\n",
    "print(\"\\nTesting the train_loader...\")\n",
    "try:\n",
    "    # Get one batch\n",
    "    anchor_batch, positive_batch, negative_batch = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"Anchor batch shape: {anchor_batch.shape}\")\n",
    "    print(f\"Positive batch shape: {positive_batch.shape}\")\n",
    "    print(f\"Negative batch shape: {negative_batch.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please check your CUB_ROOT_DIR path and dataset integrity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c31a04-f4c0-4a11-bc61-b4c880a60877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A network that uses a pretrained ResNet18 backbone\n",
    "    to extract embeddings from images.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim (int): The dimension of the output embedding.\n",
    "        \"\"\"\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        \n",
    "        # 1. Load the pretrained ResNet18\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # 2. Get the number of features from the layer *before* the classifier\n",
    "        # In ResNet18, this layer is named 'fc'\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # 3. Replace the final classifier layer ('fc') with our new embedding layer\n",
    "        # We just use a simple linear layer.\n",
    "        self.backbone.fc = nn.Linear(num_features, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input batch of images [B, C, H, W]\n",
    "        Returns:\n",
    "            torch.Tensor: Embeddings [B, embedding_dim]\n",
    "        \"\"\"\n",
    "        # 1. Pass image through the modified ResNet\n",
    "        embeddings = self.backbone(x)\n",
    "        \n",
    "        # 2. Normalize the embeddings (L2 normalization)\n",
    "        # This is the \"Normalization (Optional)\" step.\n",
    "        # It makes all embedding vectors have a length of 1,\n",
    "        # which is standard practice for triplet loss.\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# 1. Initialize the model\n",
    "embedding_dim = 128\n",
    "model = EmbeddingNet(embedding_dim)\n",
    "\n",
    "# 2. Check if a GPU is available and move the model to it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(model)\n",
    "\n",
    "# 3. Test the model with one batch from your loader\n",
    "try:\n",
    "    anchor_batch, positive_batch, negative_batch = next(iter(train_loader))\n",
    "    \n",
    "    # Move batch to the same device as the model\n",
    "    anchor_batch = anchor_batch.to(device)\n",
    "    \n",
    "    # Perform a forward pass\n",
    "    model.eval() # Set model to evaluation mode for testing\n",
    "    with torch.no_grad(): # Don't calculate gradients\n",
    "        embeddings = model(anchor_batch)\n",
    "        \n",
    "    print(f\"\\nSuccessfully passed one batch through the model.\")\n",
    "    print(f\"Input batch shape: {anchor_batch.shape}\")\n",
    "    print(f\"Output embedding shape: {embeddings.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError testing model with data batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729145c0-7a18-4f91-81bb-579f07ba0c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "from tqdm import tqdm # A nice library for progress bars\n",
    "\n",
    "# --- 1. Setup Optimizer and Loss Function ---\n",
    "\n",
    "# Your project specifies a margin of 1.0 [cite: 21, 60]\n",
    "margin = 1.0\n",
    "loss_fn = nn.TripletMarginLoss(margin=margin)\n",
    "\n",
    "# The project suggests experimenting. Adam is a great start.\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# --- 2. Training Function ---\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Handles the training logic for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    for (anchor_img, positive_img, negative_img) in tqdm(train_loader, desc=\"Training\"):\n",
    "        # Move data to the device\n",
    "        anchor_img = anchor_img.to(device)\n",
    "        positive_img = positive_img.to(device)\n",
    "        negative_img = negative_img.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get embeddings\n",
    "        anchor_emb = model(anchor_img)\n",
    "        positive_emb = model(positive_img)\n",
    "        negative_emb = model(negative_img)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# --- 3. Validation Function ---\n",
    "\n",
    "def validate_one_epoch(model, test_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Handles the validation logic for one epoch.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # No gradients needed for validation\n",
    "        for (anchor_img, positive_img, negative_img) in tqdm(test_loader, desc=\"Validating\"):\n",
    "            # Move data to the device\n",
    "            anchor_img = anchor_img.to(device)\n",
    "            positive_img = positive_img.to(device)\n",
    "            negative_img = negative_img.to(device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            anchor_emb = model(anchor_img)\n",
    "            positive_emb = model(positive_img)\n",
    "            negative_emb = model(negative_img)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    return running_loss / len(test_loader)\n",
    "\n",
    "# --- 4. Main Training Loop ---\n",
    "\n",
    "num_epochs = 10 # You can start with 10-20 and see how it goes\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate_one_epoch(model, test_loader, loss_fn, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Time: {end_time - start_time:.2f}s - \"\n",
    "          f\"Train Loss: {train_loss:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# You can now use `train_losses` and `val_losses` to plot your training curves\n",
    "# as required for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901a94f-85b2-41be-ab18-05c4bd8f4219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The 'train_losses' and 'val_losses' variables are from your training loop\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_losses, 'b-o', label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses, 'r-o', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec53447-f4b2-4f5b-a3ad-3292e9a377bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUBEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that returns just one image and its label.\n",
    "    Used for generating embeddings for the entire test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='test', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(self.root_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        # Load metadata (same as before)\n",
    "        images_df = pd.read_csv(os.path.join(self.root_dir, 'images.txt'), sep=' ', names=['img_id', 'filepath'])\n",
    "        labels_df = pd.read_csv(os.path.join(self.root_dir, 'image_class_labels.txt'), sep=' ', names=['img_id', 'class_id'])\n",
    "        split_df = pd.read_csv(os.path.join(self.root_dir, 'train_test_split.txt'), sep=' ', names=['img_id', 'is_train'])\n",
    "        \n",
    "        data_df = images_df.merge(labels_df, on='img_id').merge(split_df, on='img_id')\n",
    "        data_df['class_id'] = data_df['class_id'] - 1 # 0-indexed\n",
    "        \n",
    "        # Filter for first 10 classes\n",
    "        data_df = data_df[data_df['class_id'] < 10].reset_index(drop=True)\n",
    "        \n",
    "        # Filter by split\n",
    "        target_split = 1 if self.split == 'train' else 0\n",
    "        self.data_df = data_df[data_df['is_train'] == target_split].reset_index(drop=True)\n",
    "        \n",
    "        self.data_list = list(zip(self.data_df['filepath'], self.data_df['class_id']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def _load_image(self, filepath):\n",
    "        full_path = os.path.join(self.image_dir, filepath)\n",
    "        img = Image.open(full_path).convert('RGB')\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the image and its label\n",
    "        img_path, label = self.data_list[index]\n",
    "        img = self._load_image(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a77937-84cb-4804-bf39-4607b35732b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same transforms as before\n",
    "eval_dataset = CUBEmbeddingDataset(\n",
    "    root_dir=CUB_ROOT_DIR,\n",
    "    split='test',\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False, # No shuffling!\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# --- Function to get all embeddings ---\n",
    "def get_all_embeddings(model, loader, device):\n",
    "    model.eval() # Set model to eval mode\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Getting Test Embeddings\"):\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            \n",
    "    # Concatenate all batches\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    return all_embeddings, all_labels\n",
    "\n",
    "# Get the embeddings and labels for your test set\n",
    "test_embeddings, test_labels = get_all_embeddings(model, eval_loader, device)\n",
    "\n",
    "print(f\"\\nShape of test embeddings: {test_embeddings.shape}\")\n",
    "print(f\"Shape of test labels: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132a0ba-ca32-43f6-84b1-8d3fee915f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 1. Create the t-SNE model\n",
    "# n_components=2 means we are projecting down to 2 dimensions\n",
    "tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n",
    "\n",
    "# 2. Fit and transform your embeddings\n",
    "# This can take a minute\n",
    "print(\"Running t-SNE... (this may take a moment)\")\n",
    "tsne_embeddings = tsne.fit_transform(test_embeddings)\n",
    "\n",
    "print(\"t-SNE finished.\")\n",
    "\n",
    "# 3. Plot the results\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Plot each class (0-9) with a different color\n",
    "for i in range(10): # We have 10 classes\n",
    "    indices = (test_labels == i)\n",
    "    plt.scatter(\n",
    "        tsne_embeddings[indices, 0], \n",
    "        tsne_embeddings[indices, 1], \n",
    "        alpha=0.6,\n",
    "        label=f'Class {i+1}'\n",
    "    )\n",
    "\n",
    "plt.title('t-SNE Visualization of Test Set Embeddings')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303990e-f275-43d0-811a-332ca90d2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_precision_at_k(embeddings, labels, k=5):\n",
    "    \"\"\"\n",
    "    Calculates Precision@k for a given set of embeddings and labels.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): The NxD tensor of embeddings.\n",
    "        labels (torch.Tensor): The N-dimensional tensor of labels.\n",
    "        k (int): The number of top-k neighbors to consider.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculate the pairwise distance matrix\n",
    "    # We use torch.cdist for an efficient L2 (Euclidean) distance calculation\n",
    "    # The result is an NxN matrix where dist_matrix[i, j] is the distance\n",
    "    # between embedding i and embedding j.\n",
    "    dist_matrix = torch.cdist(embeddings, embeddings, p=2)\n",
    "    \n",
    "    # 2. Get the indices of the top-k + 1 closest neighbors\n",
    "    # We ask for k + 1 because the 1st closest neighbor (at index 0)\n",
    "    # will always be the image itself (distance = 0), which we must ignore.\n",
    "    # `largest=False` means we want the *smallest* distances.\n",
    "    _, top_k_indices = torch.topk(dist_matrix, k + 1, dim=1, largest=False)\n",
    "    \n",
    "    # 3. Ignore the first column (the self-match)\n",
    "    # We are left with an Nxk tensor of neighbor indices.\n",
    "    top_k_neighbors = top_k_indices[:, 1:]\n",
    "    \n",
    "    # 4. Get the labels of these top-k neighbors\n",
    "    # We use the 'labels' tensor to look up the class of each neighbor.\n",
    "    # `retrieved_labels` will be an Nxk tensor.\n",
    "    retrieved_labels = torch.gather(labels.unsqueeze(0).expand(top_k_neighbors.shape[0], -1), 1, top_k_neighbors)\n",
    "\n",
    "    # 5. Compare the retrieved labels to the query labels\n",
    "    # We reshape the query labels to (N, 1) for broadcasting.\n",
    "    # `matches` will be an Nxk boolean tensor.\n",
    "    query_labels = labels.unsqueeze(1)\n",
    "    matches = (retrieved_labels == query_labels)\n",
    "    \n",
    "    # 6. Calculate precision for each query\n",
    "    # We sum the matches (True=1, False=0) for each query and divide by k.\n",
    "    precision_per_query = matches.sum(dim=1).float() / k\n",
    "    \n",
    "    # 7. Calculate the average precision across all queries\n",
    "    average_precision = precision_per_query.mean().item()\n",
    "    \n",
    "    return average_precision\n",
    "\n",
    "# --- Run the calculation ---\n",
    "# You can try a few different values for k\n",
    "\n",
    "# Using k=1 (Precision@1)\n",
    "# This checks if the *single closest* image has the same label.\n",
    "p_at_1 = calculate_precision_at_k(test_embeddings, test_labels, k=1)\n",
    "print(f\"Precision@1: {p_at_1 * 100:.2f}%\")\n",
    "\n",
    "# Using k=5 (Precision@5)\n",
    "p_at_5 = calculate_precision_at_k(test_embeddings, test_labels, k=5)\n",
    "print(f\"Precision@5: {p_at_5 * 100:.2f}%\")\n",
    "\n",
    "# Using k=10 (Precision@10)\n",
    "p_at_10 = calculate_precision_at_k(test_embeddings, test_labels, k=10)\n",
    "print(f\"Precision@10: {p_at_10 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d79ba-37c9-4426-b6fb-bd77b87b284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap # Make sure this is imported\n",
    "\n",
    "# 1. Create the UMAP model\n",
    "# We use the same parameters as the baseline for a fair comparison\n",
    "print(\"Running UMAP on trained embeddings...\")\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "\n",
    "# 2. Fit and transform your *trained* test embeddings\n",
    "# 'test_embeddings' is the variable holding your trained embeddings\n",
    "umap_embeddings_trained = umap_reducer.fit_transform(test_embeddings)\n",
    "\n",
    "print(\"UMAP finished.\")\n",
    "\n",
    "# 3. Plot the results\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Plot each class (1-10) with a different color\n",
    "for i in range(10):\n",
    "    indices = (test_labels == i)\n",
    "    plt.scatter(\n",
    "        umap_embeddings_trained[indices, 0], \n",
    "        umap_embeddings_trained[indices, 1], \n",
    "        alpha=0.6,\n",
    "        label=f'Class {i + 1}' \n",
    "    )\n",
    "\n",
    "plt.title('UMAP Visualization of Test Set Embeddings (After Training)')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d226116-ff7a-45f7-a4f3-1f31e2735bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
