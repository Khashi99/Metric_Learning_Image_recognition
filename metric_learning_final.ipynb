{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!curl -LO https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz\n!curl -LO https://data.caltech.edu/records/w9d68-gec53/files/segmentations.tgz","metadata":{"id":"F3BLKE89hJ5R","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tar -xzf CUB_200_2011.tgz\n!tar -xzf segmentations.tgz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install umap-learn","metadata":{"id":"7kXwE7zljhft","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check device (whether NVIDIA or AMD)\nimport torch\nprint(torch.cuda.is_available())\nprint(torch.cuda.device_count())\nprint(torch.cuda.current_device())\nprint(torch.cuda.get_device_name())\nprint(torch.cuda.get_device_properties())\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"nkeX7T4U79kL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom collections import defaultdict\n\nCURR_DIR = os.getcwd()\nBASE_DIR = os.path.join(CURR_DIR, 'CUB_200_2011')\nATTRIBUTES_FILE = 'attributes.txt'\nIMAGE_LABELS_FILE = os.path.join(BASE_DIR, 'attributes', 'image_attribute_labels.txt')\nIMAGES_FILE = os.path.join(BASE_DIR, 'images.txt')\n# set seed (for reproducibility)\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# --- load attributes-to-name ---\nattribute_id_to_name = {}\nprint(\"Loading attribute definitions...\")\nwith open(ATTRIBUTES_FILE, 'r') as f:\n    for line in f:\n        parts = line.strip().split(maxsplit=1)\n        if len(parts) == 2:\n            attr_id, attr_name = parts\n            attribute_id_to_name[attr_id] = attr_name\nprint(f\"-> Loaded {len(attribute_id_to_name)} attribute definitions.\")\n\n# --- load image-id-to attributes mappping ---\nimage_to_attributes = defaultdict(list)\nprint(\"Loading image attributes...\")\nwith open(IMAGE_LABELS_FILE, 'r') as f:\n    for line in f:\n        parts = line.strip().split()\n        image_id, attr_id, is_present = parts[0], parts[1], parts[2]\n\n        if is_present == '1':\n            attr_name = attribute_id_to_name.get(attr_id, \"Unknown\")\n            image_to_attributes[image_id].append((attr_id, attr_name))\nprint(f\"-> Loaded attributes for {len(image_to_attributes)} images.\")\n\n# --- load image ID to file path mapping ---\nimage_id_to_path = {}\nprint(\"Loading image paths...\")\nwith open(IMAGES_FILE, 'r') as f:\n    for line in f:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_id, image_path = parts[0], parts[1]\n            image_id_to_path[image_id] = image_path\nprint(f\"-> Loaded {len(image_id_to_path)} image paths.\")\n\n# select random images\nall_image_ids = list(image_id_to_path.keys())\nrandom_image_ids = random.sample(all_image_ids, 2)\n# print(f\"\\nSelected 9 random image IDs: {random_image_ids}\")\n\nprint(\"Generating plot...\")\nfig, axes = plt.subplots(1, 2, figsize=(15, 18))\nfig.subplots_adjust(hspace=0.5, wspace=0.1)\naxes = axes.flatten()\n\nfor i, image_id in enumerate(random_image_ids):\n    ax = axes[i]\n\n    # get image data\n    image_relative_path = image_id_to_path[image_id]\n    image_full_path = os.path.join(BASE_DIR, 'images', image_relative_path)\n    image_name = os.path.basename(image_relative_path)\n\n    # get class ID\n    class_folder = os.path.dirname(image_relative_path)\n    class_id = int(class_folder.split('.')[0])\n\n    # get attribute data\n    attributes = image_to_attributes.get(image_id, [])\n    attr_list = [name for id, name in attributes[:4]]\n    attr_string = \"\\n\".join([f\"- {a}\" for a in attr_list])\n    if len(attributes) > 4:\n        attr_string += \"\\n- ...\"\n\n    img = mpimg.imread(image_full_path)\n    ax.imshow(img)\n    ax.axis('off')\n\n    title = f\"Class #: {class_id} | Image #: {image_id}\\n{image_name}\"\n    ax.set_title(title, fontsize=9, wrap=True)\n\n    ax.text(0, -0.05, attr_string,\n            transform=ax.transAxes,\n            fontsize=8,\n            verticalalignment='top')\n\n# save plot to file and display\noutput_filename = 'cub_attributes_grid.png'\nplt.savefig(output_filename)\nprint(f\"\\n created and saved plot to '{output_filename}'\")\nplt.show()","metadata":{"id":"r-6-jWAwPSIM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#UMAP and T-SNE\nfrom sklearn.manifold import TSNE\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib\nimport umap\n\ndef umap_tsne(model, embedding, labels, plot_title, output_file_name):\n  \"\"\"\n  This fucntion is used for UMAP, TSNE before and after the training\n  \"\"\"\n\n  print(f\"Got {embedding.shape[0]} baseline embeddings.\")\n\n  # Run t-SNE\n  tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n  tsne_embeddings = tsne.fit_transform(embedding)\n  print(\"t-SNE finished.\")\n\n  # Run UMAP\n  umap_reducer = umap.UMAP(n_components=2, random_state=42)\n  umap_embeddings = umap_reducer.fit_transform(embedding)\n  print(\"UMAP finished.\")\n\n  # Plot both\n  fig, axes = matplotlib.pyplot.subplots(1, 2, figsize=(20, 10))\n\n  # t-SNE Plot\n  axes[0].set_title('t-SNE of Embeddings')\n  for i in range(10): # We have 10 classes\n      indices = (labels == i)\n      axes[0].scatter(\n          tsne_embeddings[indices, 0],\n          tsne_embeddings[indices, 1],\n          alpha=0.6,\n          label=f'Class {i+1}'\n      )\n  axes[0].set_xlabel('t-SNE Component 1')\n  axes[0].set_ylabel('t-SNE Component 2')\n  axes[0].legend()\n\n  # UMAP Plot\n  axes[1].set_title('UMAP of Embeddings')\n  for i in range(10): # We have 10 classes\n      indices = (labels == i)\n      axes[1].scatter(\n          umap_embeddings[indices, 0],\n          umap_embeddings[indices, 1],\n          alpha=0.6,\n          label=f'Class {i+1}'\n      )\n  axes[1].set_xlabel('UMAP Component 1')\n  axes[1].set_ylabel('UMAP Component 2')\n  axes[1].legend()\n\n  matplotlib.pyplot.suptitle(plot_title, fontsize=16)\n  plt.savefig(output_file_name)\n  \n  matplotlib.pyplot.show()\n  matplotlib.pyplot.close()\n\n  print(f\"\\n created and saved plot to '{output_file_name}'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn\nimport torch.nn.functional\nimport torchvision.models\n\nclass EmbeddingNet(torch.nn.Module):\n    \"\"\"\n    A network that uses a pretrained ResNet18 backbone to extract embeddings from images. Used for visualizations with UMAP / t-SNE\n    \"\"\"\n    def __init__(self, embedding_dim=128, backbone_name='resnet18'):\n        super(EmbeddingNet, self).__init__()\n\n        # Load the pretrained ResNet18\n        if backbone_name == 'resnet18':\n            self.backbone = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n        elif backbone_name == 'resnet34':\n            self.backbone = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.DEFAULT)\n        elif backbone_name == 'resnet50':\n            self.backbone = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n        else:\n            raise ValueError(f\"Unsupported backbone: {backbone_name}. Please choose 'resnet18', 'resnet34', or 'resnet50'.\")\n\n        # get the number of features from the layer *before* the classifier\n        num_features = self.backbone.fc.in_features\n\n        # Replace the final classifier layer ('fc') with our new embedding layer\n        self.backbone.fc = torch.nn.Linear(num_features, embedding_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the network.\n        \"\"\"\n        # pass image through the modified ResNet\n        embeddings = self.backbone(x)\n\n        # normalize the embeddings (L2 normalization) to make the embedding vectors have a length of 1, standard for triplet loss.\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n        return embeddings","metadata":{"id":"aYH7b8FLRExF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_all_embeddings(model, loader, device):\n    model.eval() # Set model to eval mode\n    all_embeddings = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in tqdm(loader, desc=\"Getting Embeddings\"):\n            images = images.to(device)\n            embeddings = model(images)\n\n            all_embeddings.append(embeddings.cpu())\n            all_labels.append(labels.cpu())\n\n    # Concatenate all batches\n    all_embeddings = torch.cat(all_embeddings, dim=0)\n    all_labels = torch.cat(all_labels, dim=0)\n\n    return all_embeddings, all_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm # library for progress bars\nfrom torchvision import transforms\nimport pandas\n\nclass CUBEmbeddingDataset(Dataset):\n    \"\"\"\n    A dataset that returns just one image and its label.\n    Used for generating embeddings for the entire test set (precision@k)\n    returns (image, label) pairs\n    \"\"\"\n    def __init__(self, root_dir, split='test', transform=None):\n        self.root_dir = root_dir\n        self.image_dir = os.path.join(self.root_dir, 'images')\n        self.transform = transform\n        self.split = split\n\n        # Load metadata (same as before)\n        images_df = pandas.read_csv(os.path.join(self.root_dir, 'images.txt'), sep=' ', names=['img_id', 'filepath'])\n        labels_df = pandas.read_csv(os.path.join(self.root_dir, 'image_class_labels.txt'), sep=' ', names=['img_id', 'class_id'])\n        split_df = pandas.read_csv(os.path.join(self.root_dir, 'train_test_split.txt'), sep=' ', names=['img_id', 'is_train'])\n\n        data_df = images_df.merge(labels_df, on='img_id').merge(split_df, on='img_id')\n        data_df['class_id'] = data_df['class_id'] - 1 # 0-indexed\n\n        # Filter for first 10 classes. this should be removed for the real datase\n        data_df = data_df[data_df['class_id'] < 10].reset_index(drop=True)\n\n        # Filter by split\n        target_split = 1 if self.split == 'train' else 0\n        self.data_df = data_df[data_df['is_train'] == target_split].reset_index(drop=True)\n\n        self.data_list = list(zip(self.data_df['filepath'], self.data_df['class_id']))\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def _load_image(self, filepath):\n        full_path = os.path.join(self.image_dir, filepath)\n        img = Image.open(full_path).convert('RGB')\n        return img\n\n    def __getitem__(self, index):\n        # Get the image and its label\n        img_path, label = self.data_list[index]\n        img = self._load_image(img_path)\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label","metadata":{"id":"nVFucaBNikFG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TripletCUBDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Custom DataLoader to use CUB-200 for triplet loss.\n    \"\"\"\n    def __init__(self, root_dir, split='train', transform=None):\n        self.root_dir = root_dir\n        self.image_dir = os.path.join(self.root_dir, 'images')\n        self.transform = transform\n        self.split = split\n        # Load metadata\n        self._load_metadata()\n\n    def _load_metadata(self):\n        # Read images.txt: <image_id> <filepath>\n        images_df = pandas.read_csv(\n            os.path.join(self.root_dir, 'images.txt'),\n            sep=' ',\n            names=['img_id', 'filepath']\n        )\n\n        # Read image_class_labels.txt: <image_id> <class_id>\n        labels_df = pandas.read_csv(\n            os.path.join(self.root_dir, 'image_class_labels.txt'),\n            sep=' ',\n            names=['img_id', 'class_id']\n        )\n\n        # Read train_test_split.txt: <image_id> <is_train>\n        split_df = pandas.read_csv(\n            os.path.join(self.root_dir, 'train_test_split.txt'),\n            sep=' ',\n            names=['img_id', 'is_train']\n        )\n\n        # Merge dataframes\n        data_df = images_df.merge(labels_df, on='img_id').merge(split_df, on='img_id')\n\n        data_df['class_id'] = data_df['class_id'] - 1\n\n        # Filter by split (1 for train, 0 for test)\n        target_split = 1 if self.split == 'train' else 0\n        self.data_df = data_df[data_df['is_train'] == target_split].reset_index(drop=True)\n\n        # Create a list of all data points (filepath, class_id)\n        self.data_list = list(zip(self.data_df['filepath'], self.data_df['class_id']))\n\n        # Create a dictionary mapping class_id -> [list of indices in self.data_list]\n        self.class_to_indices = {}\n        for idx, (_, class_id) in enumerate(self.data_list):\n            if class_id not in self.class_to_indices:\n                self.class_to_indices[class_id] = []\n            self.class_to_indices[class_id].append(idx)\n\n        # Store a list of all unique class IDs\n        self.classes = list(self.class_to_indices.keys())\n\n    def __len__(self):\n        return len(self.data_list)\n\n    # def _load_image(self, filepath):\n    #     \"\"\"Helper to load an image from its relative path.\"\"\"\n    #     full_path = os.path.join(self.image_dir, filepath)\n    #     img = Image.open(full_path).convert('RGB')\n    #     return img\n\n    def _load_image(self, filepath):\n      \"\"\"Helper to load an image from its relative path.\"\"\"\n      full_path = os.path.join(self.image_dir, filepath)\n\n      try:\n          img = Image.open(full_path).convert('RGB')\n          return img\n      except OSError as e:\n          print(f\"!!!!!!!!!!!!!! ERROR LOADING IMAGE !!!!!!!!!!!!!!\")\n          print(f\"Failed to load image: {full_path}\")\n          print(f\"Error: {e}\")\n          raise e\n\n    def __getitem__(self, index):\n        \"\"\"\n        Generate a triplet!\n        \"\"\"\n\n        # get the ANCHOR\n        anchor_path, anchor_class = self.data_list[index]\n        anchor_img = self._load_image(anchor_path)\n\n        # get a POSITIVE image (same class, different image)\n        positive_indices = self.class_to_indices[anchor_class]\n\n        # Ensure we don't pick the same image as the anchor\n        positive_index = index\n        while positive_index == index and len(positive_indices) > 1:\n            positive_index = random.choice(positive_indices)\n\n        positive_path, _ = self.data_list[positive_index]\n        positive_img = self._load_image(positive_path)\n\n        # get NEGATIVE image (different class)\n        negative_class = anchor_class\n        while negative_class == anchor_class:\n            negative_class = random.choice(self.classes)\n\n        negative_indices = self.class_to_indices[negative_class]\n        negative_index = random.choice(negative_indices)\n\n        negative_path, _ = self.data_list[negative_index]\n        negative_img = self._load_image(negative_path)\n\n        # apply transforms\n        if self.transform:\n            anchor_img = self.transform(anchor_img)\n            positive_img = self.transform(positive_img)\n            negative_img = self.transform(negative_img)\n\n        return anchor_img, positive_img, negative_img","metadata":{"id":"sPY1P8b5jdBy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm import tqdm # library for progress bars\nfrom torchvision import transforms\nimport pandas\n\nEPOCHS=2\nBATCH_SIZE=64\nLEARN_RATE=0.001\nNUM_WORKERS=0\n\n# train transforms (additional augmentations)\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5), # % of images\n    transforms.RandomRotation(10), # rotates by up to x degrees\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n# test transforms (no additional augmentatations)\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n## for tiplet-loss\n# create datasets\ntrain_dataset = TripletCUBDataset(\n    root_dir=BASE_DIR,\n    split='train',\n    transform=train_transforms\n)\n\ntest_dataset = TripletCUBDataset(\n    root_dir=BASE_DIR,\n    split='test',\n    transform=test_transforms\n)\n\n# create dataloaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS\n)\n\n## for embedding evaluation (precision@k)\neval_dataset = CUBEmbeddingDataset(\n    root_dir=BASE_DIR,\n    split='test',\n    transform=test_transforms\n)\n\neval_loader = DataLoader(\n    eval_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS\n)\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")\nprint(f\"Eval dataset size (for embeddings): {len(eval_dataset)}\")\n\n# test loader\nprint(\"\\nTesting the train_loader...\")\ntry:\n    # Get one batch\n    anchor_batch, positive_batch, negative_batch = next(iter(train_loader))\n    print(f\"Anchor batch shape: {anchor_batch.shape}\")\n    print(f\"Positive batch shape: {positive_batch.shape}\")\n    print(f\"Negative batch shape: {negative_batch.shape}\")\nexcept Exception as e:\n    print(f\"Error loading data: {e}\")","metadata":{"id":"B7zcwVAQlb3z","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm\nimport torch.optim\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Helpers: batch cosine stats\ndef batch_cosine_stats(a, p, n):\n    \"\"\"\n    a, p, n are L2-normalized torch embeddings of shape (B, D).\n    Returns numpy arrays sim_ap, sim_an (shape B,)\n    and batch_top1 (float in [0,1]).\n    \"\"\"\n    A = a.detach().cpu().numpy()\n    P = p.detach().cpu().numpy()\n    N = n.detach().cpu().numpy()\n\n    sim_ap = cosine_similarity(A, P).diagonal()\n    sim_an = cosine_similarity(A, N).diagonal()\n    top1 = (sim_ap > sim_an).mean()\n    return sim_ap, sim_an, float(top1)\n\n# Training\ndef train(model, train_loader, optimizer, loss_fn, device):\n    model.train()\n    running_loss = 0.0\n\n    # epoch accumulators\n    sum_ap = 0.0\n    sum_an = 0.0\n    sum_top1 = 0.0\n    n_examples = 0\n\n    for (anchor_img, positive_img, negative_img) in tqdm(train_loader, desc=\"Training\"):\n        # Move data to device\n        anchor_img  = anchor_img.to(device, non_blocking=True)\n        positive_img= positive_img.to(device, non_blocking=True)\n        negative_img= negative_img.to(device, non_blocking=True)\n\n        optimizer.zero_grad()\n\n        # Forward to get embeddings\n        anchor_emb  = model(anchor_img)     # (B, D)\n        positive_emb= model(positive_img)   # (B, D)\n        negative_emb= model(negative_img)   # (B, D)\n\n        # L2-normalize so cosine â‰ˆ dot product\n        a = torch.nn.functional.normalize(anchor_emb,  p=2, dim=1)\n        p = torch.nn.functional.normalize(positive_emb,p=2, dim=1)\n        n = torch.nn.functional.normalize(negative_emb,p=2, dim=1)\n\n        # Batch cosine stats (before mining)\n        sim_ap, sim_an, batch_top1 = batch_cosine_stats(a, p, n)\n        B = a.size(0)\n        sum_ap   += float(sim_ap.sum())\n        sum_an   += float(sim_an.sum())\n        sum_top1 += batch_top1 * B\n        n_examples += B\n\n        loss = loss_fn(a, p, n)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += float(loss.item())\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_mean_ap = sum_ap / n_examples\n    epoch_mean_an = sum_an / n_examples\n    epoch_top1    = sum_top1 / n_examples\n\n    return {\n        \"loss\": epoch_loss,\n        \"mean_ap\": epoch_mean_ap,\n        \"mean_an\": epoch_mean_an,\n        \"top1\": epoch_top1,\n        \"count\": n_examples,\n    }","metadata":{"id":"jsPdGT7tmo4l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setup for each model; ResNet18 and ResNet34\nembedding_dim = 512\nmargin = 1.0\nloss_fn = torch.nn.TripletMarginLoss(margin=margin)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# initialize ResNet18\nmodel_18 = EmbeddingNet(embedding_dim, backbone_name='resnet18')\nmodel_18.to(device)\noptimizer = torch.optim.Adam(model_18.parameters(), lr=LEARN_RATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ResNet18 training loop\nprint(\"Starting ResNet18 training...\")\nprint(f\"Selected: \\n epochs: {EPOCHS} \\n batch size: {BATCH_SIZE} \\n learn rate: {LEARN_RATE}\")\ntrain_hist = []\nfor epoch in range(EPOCHS):\n    start_time = time.time()\n\n    tr = train(model_18, train_loader, optimizer, loss_fn, device)\n\n    train_hist.append(tr)\n\n    elapsed = time.time() - start_time\n    print(\n        f\"Epoch {epoch+1}/{EPOCHS} | {elapsed:.2f}s | \"\n        f\"Train Loss: {tr['loss']:.4f} | \"\n        f\"Train cos(AP): {tr['mean_ap']:.3f} | \"\n        f\"Train cos(AN): {tr['mean_an']:.3f} | \"\n        f\"Train Top1: {tr['top1']:.3f} \"\n    )\n    print(\"\\n\")\n\nprint(\"ResNet18 training finished.\")\n\n# save model's weight and parameters\nMODEL_PATH_18 = 'resnet18_model.pth'\ntorch.save(model_18.state_dict(), MODEL_PATH_18)\n\nprint(f\"Model state_dict saved to {MODEL_PATH_18}\")","metadata":{"id":"uWkseZREsEhF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# initialize ResNet34\nmodel_34 = EmbeddingNet(embedding_dim, backbone_name='resnet34')\nmodel_34.to(device)\noptimizer = torch.optim.Adam(model_34.parameters(), lr=LEARN_RATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ResNet34 training loop\nprint(\"Starting ResNet34 training...\")\nprint(f\"Selected: \\n epochs: {EPOCHS} \\n batch size: {BATCH_SIZE} \\n learn rate: {LEARN_RATE}\")\ntrain_hist = []\nfor epoch in range(EPOCHS):\n    start_time = time.time()\n\n    tr = train(model_34, train_loader, optimizer, loss_fn, device)\n\n    train_hist.append(tr)\n\n    elapsed = time.time() - start_time\n    print(\n        f\"Epoch {epoch+1}/{EPOCHS} | {elapsed:.2f}s | \"\n        f\"Train Loss: {tr['loss']:.4f} | \"\n        f\"Train cos(AP): {tr['mean_ap']:.3f} | \"\n        f\"Train cos(AN): {tr['mean_an']:.3f} | \"\n        f\"Train Top1: {tr['top1']:.3f} \"\n    )\n    print(\"\\n\")\n\nprint(\"ResNet34 training finished.\")\n\n# save model's weight and parameters\nMODEL_PATH_34 = 'resnet34_model.pth'\ntorch.save(model_34.state_dict(), MODEL_PATH_34)\n\nprint(f\"Model state_dict saved to {MODEL_PATH_34}\")","metadata":{"id":"m0-6n8FIsjEH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# manage GPU memory\nimport torch\nimport torch.nn as nn\n\ndef memory_stats():\n    print(torch.cuda.memory_allocated()/1024**2)\n    print(torch.cuda.memory_reserved()/1024**2)\n    \n# def allocate():\n#     x = torch.randn(1024*1024, device='cuda')\n#     memory_stats()\n\n# import gc\nmemory_stats() \n# gc.collect()\n# del model_18\n# del model_34\n# del optimizer\ntorch.cuda.empty_cache()\nmemory_stats()\nprint(\"Freed GPU memory.\")","metadata":{"id":"ELsJUWirTOLF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# loss function plot\nloss_hist = [d['loss'] for d in train_hist]\n\n# x-axis based on actual length (number of epochs)\nepochs_range = range(1, len(loss_hist) + 1)\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(epochs_range, loss_hist, marker='o', label='Training Loss')\nplt.title('Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\n\n# save plot to file and display\ntraining_loss_graph = 'training_loss_graph.png'\nplt.savefig(training_loss_graph)\n\nprint(f\"\\n created and saved plot to '{training_loss_graph}'\")\n\nplt.show()","metadata":{"id":"0kQhA_u0wlXy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test model\ndef test(model, test_loader, loss_fn, device):\n    \"\"\"\n    Runs the model on the test set and returns a dictionary of metrics.\n    \"\"\"\n    model.eval()\n    running_loss = 0.0\n\n    # epoch accumulators\n    sum_ap = 0.0\n    sum_an = 0.0\n    sum_top1 = 0.0\n    n_examples = 0\n\n    # disable gradient calculations\n    with torch.no_grad():\n        for (anchor_img, positive_img, negative_img) in tqdm(test_loader, desc=\"Testing\"):\n            # move data to device\n            anchor_img  = anchor_img.to(device, non_blocking=True)\n            positive_img= positive_img.to(device, non_blocking=True)\n            negative_img= negative_img.to(device, non_blocking=True)\n\n            # get embeddings\n            anchor_emb  = model(anchor_img)\n            positive_emb= model(positive_img)\n            negative_emb= model(negative_img)\n\n            # L2-normalize already in EmbeddingNet (forward method)\n\n            # Batch cosine stats\n            sim_ap, sim_an, batch_top1 = batch_cosine_stats(anchor_emb, positive_emb, negative_emb)\n            B = anchor_emb.size(0)\n            sum_ap   += float(sim_ap.sum())\n            sum_an   += float(sim_an.sum())\n            sum_top1 += batch_top1 * B\n            n_examples += B\n\n            loss = loss_fn(anchor_emb, positive_emb, negative_emb)\n            running_loss += float(loss.item())\n\n    # final epoch metrics\n    epoch_loss = running_loss / len(test_loader)\n    epoch_mean_ap = sum_ap / n_examples\n    epoch_mean_an = sum_an / n_examples\n    epoch_top1    = sum_top1 / n_examples\n\n    return {\n        \"loss\": epoch_loss,\n        \"mean_ap\": epoch_mean_ap,\n        \"mean_an\": epoch_mean_an,\n        \"top1\": epoch_top1,\n        \"count\": n_examples,\n    }","metadata":{"id":"bPyH6ZbCInpK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test on ResNet18\n# using same margin and loss_fn from training run\nprint(\"\\nTesting ResNet18 Model...\")\n\n# initialize and load model to device\nloaded_model_18 = EmbeddingNet(embedding_dim, backbone_name='resnet18')\nloaded_model_18.to(device)\nloaded_model_18.load_state_dict(torch.load(MODEL_PATH_18, map_location=device))\n\ntest_stats_18 = test(model_18, test_loader, loss_fn, device)\n\nprint(\"--- ResNet18 Test Results ---\")\nprint(f\"  Loss: {test_stats_18['loss']:.4f}\")\nprint(f\"  Top-1 Acc: {test_stats_18['top1'] * 100:.2f}%\")\nprint(f\"  Avg. Pos Cosine: {test_stats_18['mean_ap']:.4f}\")\nprint(f\"  Avg. Neg Cosine: {test_stats_18['mean_an']:.4f}\")","metadata":{"id":"HTVnikHEhvZk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test on ResNet34\nfrom PIL import ImageFile\n# allow truncated images to load (test dataset has truncated images)\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nprint(\"\\nTesting ResNet34 Model...\")\n\n# initialize and load model to device\nloaded_model_34 = EmbeddingNet(embedding_dim, backbone_name='resnet34')\nloaded_model_34.to(device)\nloaded_model_34.load_state_dict(torch.load(MODEL_PATH_34, map_location=device))\n\ntest_stats_34 = test(model_34, test_loader, loss_fn, device)\n\nprint(\"--- ResNet34 Test Results ---\")\nprint(f\"  Loss: {test_stats_34['loss']:.4f}\")\nprint(f\"  Top-1 Acc: {test_stats_34['top1'] * 100:.2f}%\")\nprint(f\"  Avg. Pos Cosine: {test_stats_34['mean_ap']:.4f}\")\nprint(f\"  Avg. Neg Cosine: {test_stats_34['mean_an']:.4f}\")","metadata":{"id":"yQpeQXOxG7kL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# precision@k for ResNet18\n# get embeddings (using existing code before first UMAP / t-SNE plots)\nembeddings_18, labels_18 = get_all_embeddings(model_18, eval_loader, device)\nprint(f\"got ResNet18 {embeddings_18.shape[0]} embeddings with {labels_18.shape[0]} labels.\")","metadata":{"id":"8fgeT99ZhvZl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# precision@k for ResNet34\n# get embeddings (using existing code before first UMAP / t-SNE plots)\nembeddings_34, labels_34 = get_all_embeddings(model_34, eval_loader, device)\nprint(f\"got ResNet34 {embeddings_34.shape[0]} embeddings with {labels_34.shape[0]} labels.\")","metadata":{"id":"o_zAXeT14Plm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_precision_at_k(embeddings, labels, k, device):\n    \"\"\"\n    Calculates Precision@k for a given set of embeddings and labels.\n    Assumes embeddings are L2-normalized (which your model does).\n    \"\"\"\n    num_embeddings = len(embeddings)\n\n    embeddings_gpu = embeddings.to(device)\n    labels_gpu = labels.to(device)\n\n    # cosine similarity; embeddings are L2-normalized, dot product = cosine similarity\n    similarity_matrix = torch.mm(embeddings_gpu, embeddings_gpu.T)\n\n    precisions = []\n\n    for i in tqdm(range(num_embeddings), desc=f\"Calculating P@{k}\"):\n        query_label = labels_gpu[i]\n\n        similarities = similarity_matrix[i]\n\n        # sort and get top k+1 indices (ignoring self-match)\n        # largest=True gets highest similarity\n        _, top_k_indices = torch.topk(similarities, k + 1, largest=True)\n\n        # remove self-match (highest similarity)\n        top_k_indices = top_k_indices[1:]\n\n        # labels of top k retrieved images\n        retrieved_labels = labels_gpu[top_k_indices]\n\n        # count matches with query label\n        num_correct = torch.sum(retrieved_labels == query_label).item()\n\n        # precision for this query\n        precision = num_correct / k\n        precisions.append(precision)\n\n    # average precision across all queries\n    return np.mean(precisions)","metadata":{"id":"KMGnnEOO4Yfm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ensure using device = cuda (NVIDIA or AMD)\nprint(\"\\n--- ResNet18 Retrieval Results ---\")\np_at_1 = calculate_precision_at_k(embeddings_18, labels_18, k=1, device=device)\np_at_5 = calculate_precision_at_k(embeddings_18, labels_18, k=5, device=device)\np_at_10 = calculate_precision_at_k(embeddings_18, labels_18, k=10, device=device)\n\nprint(f\"  Precision@1:  {p_at_1 * 100:.2f}%\")\nprint(f\"  Precision@5:  {p_at_5 * 100:.2f}%\")\nprint(f\"  Precision@10: {p_at_10 * 100:.2f}%\")","metadata":{"id":"0Zv3-bYFhvZl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ensure using device = cuda (NVIDIA or AMD)\nprint(\"\\n--- ResNet34 Retrieval Results ---\")\np_at_1 = calculate_precision_at_k(embeddings_34, labels_34, k=1, device=device)\np_at_5 = calculate_precision_at_k(embeddings_34, labels_34, k=5, device=device)\np_at_10 = calculate_precision_at_k(embeddings_34, labels_34, k=10, device=device)\n\nprint(f\"  Precision@1:  {p_at_1 * 100:.2f}%\")\nprint(f\"  Precision@5:  {p_at_5 * 100:.2f}%\")\nprint(f\"  Precision@10: {p_at_10 * 100:.2f}%\")","metadata":{"id":"6QodAAT75-6B","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#plot after training resnet18\n# Use the `get_all_embeddings` function and `eval_loader` to get embeddings from the model\ntrained_resnet18_embeddings, trained_resnet18_labels = get_all_embeddings(model_18, eval_loader, device)\n\nprint(\"plots before trainig\")\n#fucntion call for the plots\numap_tsne(model_18, trained_resnet18_embeddings, trained_resnet18_labels, \"Visualization of images after training (ResNet18)\", 'trained_model_plot_resnet18.png')","metadata":{"id":"WuxXdknGf54s","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#plot after training resnet34\n# Use the `get_all_embeddings` function and `eval_loader` to get embeddings from the model\ntrained_resnet34_embeddings, trained_resnet34_labels = get_all_embeddings(model_34, eval_loader, device)\n\nprint(\"plots before trainig\")\n#fucntion call for the plots\numap_tsne(model_34, trained_resnet34_embeddings, trained_resnet34_labels, \"Visualization of images after training (ResNet34)\", 'trained_model_plot_resnet34.png')","metadata":{"id":"uO71VRJtqbDT","trusted":true},"outputs":[],"execution_count":null}]}