{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3BLKE89hJ5R",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!curl -LO https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz\n",
    "!curl -LO https://data.caltech.edu/records/w9d68-gec53/files/segmentations.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!tar -xzf CUB_200_2011.tgz\n",
    "!tar -xzf segmentations.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kXwE7zljhft",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-6-jWAwPSIM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from collections import defaultdict\n",
    "\n",
    "CURR_DIR = os.getcwd()\n",
    "BASE_DIR = os.path.join(CURR_DIR, 'CUB_200_2011')\n",
    "ATTRIBUTES_FILE = 'attributes.txt'\n",
    "IMAGE_LABELS_FILE = os.path.join(BASE_DIR, 'attributes', 'image_attribute_labels.txt')\n",
    "IMAGES_FILE = os.path.join(BASE_DIR, 'images.txt')\n",
    "# set seed (for reproducibility)\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# --- load attributes-to-name ---\n",
    "attribute_id_to_name = {}\n",
    "print(\"Loading attribute definitions...\")\n",
    "with open(ATTRIBUTES_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            attr_id, attr_name = parts\n",
    "            attribute_id_to_name[attr_id] = attr_name\n",
    "print(f\"-> Loaded {len(attribute_id_to_name)} attribute definitions.\")\n",
    "\n",
    "\n",
    "# --- load image-id-to attributes mappping ---\n",
    "image_to_attributes = defaultdict(list)\n",
    "print(\"Loading image attributes...\")\n",
    "with open(IMAGE_LABELS_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        image_id, attr_id, is_present = parts[0], parts[1], parts[2]\n",
    "\n",
    "        if is_present == '1':\n",
    "            attr_name = attribute_id_to_name.get(attr_id, \"Unknown\")\n",
    "            image_to_attributes[image_id].append((attr_id, attr_name))\n",
    "print(f\"-> Loaded attributes for {len(image_to_attributes)} images.\")\n",
    "\n",
    "\n",
    "# --- load image ID to file path mapping ---\n",
    "image_id_to_path = {}\n",
    "print(\"Loading image paths...\")\n",
    "with open(IMAGES_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            image_id, image_path = parts[0], parts[1]\n",
    "            image_id_to_path[image_id] = image_path\n",
    "print(f\"-> Loaded {len(image_id_to_path)} image paths.\")\n",
    "\n",
    "\n",
    "# select random images\n",
    "all_image_ids = list(image_id_to_path.keys())\n",
    "random_image_ids = random.sample(all_image_ids, 2)\n",
    "# print(f\"\\nSelected 9 random image IDs: {random_image_ids}\")\n",
    "\n",
    "# create plot\n",
    "print(\"Generating plot...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 18))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.1)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, image_id in enumerate(random_image_ids):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # get image data\n",
    "    image_relative_path = image_id_to_path[image_id]\n",
    "    image_full_path = os.path.join(BASE_DIR, 'images', image_relative_path)\n",
    "    image_name = os.path.basename(image_relative_path)\n",
    "\n",
    "    # get class ID\n",
    "    class_folder = os.path.dirname(image_relative_path)\n",
    "    class_id = int(class_folder.split('.')[0])\n",
    "\n",
    "    # get attribute data\n",
    "    attributes = image_to_attributes.get(image_id, [])\n",
    "    attr_list = [name for id, name in attributes[:4]]\n",
    "    attr_string = \"\\n\".join([f\"- {a}\" for a in attr_list])\n",
    "    if len(attributes) > 4:\n",
    "        attr_string += \"\\n- ...\"\n",
    "\n",
    "    img = mpimg.imread(image_full_path)\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "    title = f\"Class #: {class_id} | Image #: {image_id}\\n{image_name}\"\n",
    "    ax.set_title(title, fontsize=9, wrap=True)\n",
    "\n",
    "    ax.text(0, -0.05, attr_string,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=8,\n",
    "            verticalalignment='top')\n",
    "\n",
    "# save plot to file and display\n",
    "output_filename = 'cub_attributes_grid.png'\n",
    "plt.savefig(output_filename)\n",
    "\n",
    "# print(f\"\\n created and saved plot to '{output_filename}'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYH7b8FLRExF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "import torch.nn.functional\n",
    "import torchvision.models\n",
    "\n",
    "class EmbeddingNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A network that uses a pretrained ResNet18 backbone to extract embeddings from images. Used for visualizations with UMAP / t-SNE\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=128, backbone_name='resnet18'):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "\n",
    "        # Load the pretrained ResNet18\n",
    "        if backbone_name == 'resnet18':\n",
    "            self.backbone = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "        elif backbone_name == 'resnet34':\n",
    "            self.backbone = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.DEFAULT)\n",
    "        elif backbone_name == 'resnet50':\n",
    "            self.backbone = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}. Please choose 'resnet18', 'resnet34', or 'resnet50'.\")\n",
    "\n",
    "        # get the number of features from the layer *before* the classifier\n",
    "        num_features = self.backbone.fc.in_features\n",
    "\n",
    "        # Replace the final classifier layer ('fc') with our new embedding layer\n",
    "        self.backbone.fc = torch.nn.Linear(num_features, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        \"\"\"\n",
    "        # pass image through the modified ResNet\n",
    "        embeddings = self.backbone(x)\n",
    "\n",
    "        # normalize the embeddings (L2 normalization) to make the embedding vectors have a length of 1, standard for triplet loss.\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkeX7T4U79kL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# check device (whether NVIDIA or AMD)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.get_device_properties())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVFucaBNikFG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm # library for progress bars\n",
    "from torchvision import transforms\n",
    "import pandas\n",
    "\n",
    "class CUBEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that returns just one image and its label.\n",
    "    Used for generating embeddings for the entire test set.\n",
    "    returns (image, label) pairs\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, split='test', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(self.root_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        # Load metadata (same as before)\n",
    "        images_df = pandas.read_csv(os.path.join(self.root_dir, 'images.txt'), sep=' ', names=['img_id', 'filepath'])\n",
    "        labels_df = pandas.read_csv(os.path.join(self.root_dir, 'image_class_labels.txt'), sep=' ', names=['img_id', 'class_id'])\n",
    "        split_df = pandas.read_csv(os.path.join(self.root_dir, 'train_test_split.txt'), sep=' ', names=['img_id', 'is_train'])\n",
    "\n",
    "        data_df = images_df.merge(labels_df, on='img_id').merge(split_df, on='img_id')\n",
    "        data_df['class_id'] = data_df['class_id'] - 1 # 0-indexed\n",
    "\n",
    "        # Filter for first 10 classes. this should be removed for the real datase\n",
    "        data_df = data_df[data_df['class_id'] < 10].reset_index(drop=True)\n",
    "\n",
    "        # Filter by split\n",
    "        target_split = 1 if self.split == 'train' else 0\n",
    "        self.data_df = data_df[data_df['is_train'] == target_split].reset_index(drop=True)\n",
    "\n",
    "        self.data_list = list(zip(self.data_df['filepath'], self.data_df['class_id']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def _load_image(self, filepath):\n",
    "        full_path = os.path.join(self.image_dir, filepath)\n",
    "        img = Image.open(full_path).convert('RGB')\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the image and its label\n",
    "        img_path, label = self.data_list[index]\n",
    "        img = self._load_image(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "def get_all_embeddings(model, loader, device):\n",
    "    model.eval() # Set model to eval mode\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Getting Embeddings\"):\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)\n",
    "\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    return all_embeddings, all_labels\n",
    "\n",
    "# Defining the 'eval_loader' and 'data_transform'\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eval_dataset = CUBEmbeddingDataset(\n",
    "    root_dir=BASE_DIR,\n",
    "    split='test',\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcrL4yByjE2g",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#UMAP and T-SNE before the training\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import umap\n",
    "\n",
    "# Get \"baseline\" embeddings\n",
    "# print(\"--- Task 3: Baseline Visualization (Before Training) ---\")\n",
    "# print(\"Initializing a fresh, pre-trained ResNet18...\")\n",
    "\n",
    "def plot_Umap_T_SNE(model, embedding, labels, plot_title, output_file_name):\n",
    "  \"\"\"\n",
    "  This fucntion is used for UMAP, TSNE before and after the training\n",
    "  \"\"\"\n",
    "\n",
    "  print(f\"Got {embedding.shape[0]} baseline embeddings.\")\n",
    "\n",
    "\n",
    "  # Run t-SNE\n",
    "  tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n",
    "  tsne_embeddings = tsne.fit_transform(embedding)\n",
    "  print(\"t-SNE finished.\")\n",
    "\n",
    "  # Run UMAP\n",
    "  umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "  umap_embeddings = umap_reducer.fit_transform(embedding)\n",
    "  print(\"UMAP finished.\")\n",
    "\n",
    "  # Plot both\n",
    "  fig, axes = matplotlib.pyplot.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "  # t-SNE Plot\n",
    "  axes[0].set_title('t-SNE of Embeddings')\n",
    "  for i in range(10): # We have 10 classes\n",
    "      indices = (labels == i)\n",
    "      axes[0].scatter(\n",
    "          tsne_embeddings[indices, 0],\n",
    "          tsne_embeddings[indices, 1],\n",
    "          alpha=0.6,\n",
    "          label=f'Class {i+1}'\n",
    "      )\n",
    "  axes[0].set_xlabel('t-SNE Component 1')\n",
    "  axes[0].set_ylabel('t-SNE Component 2')\n",
    "  axes[0].legend()\n",
    "\n",
    "  # UMAP Plot\n",
    "  axes[1].set_title('UMAP of Embeddings')\n",
    "  for i in range(10): # We have 10 classes\n",
    "      indices = (labels == i)\n",
    "      axes[1].scatter(\n",
    "          umap_embeddings[indices, 0],\n",
    "          umap_embeddings[indices, 1],\n",
    "          alpha=0.6,\n",
    "          label=f'Class {i+1}'\n",
    "      )\n",
    "  axes[1].set_xlabel('UMAP Component 1')\n",
    "  axes[1].set_ylabel('UMAP Component 2')\n",
    "  axes[1].legend()\n",
    "\n",
    "  matplotlib.pyplot.suptitle(plot_title, fontsize=16)\n",
    "  plt.savefig(output_file_name)\n",
    "  \n",
    "  matplotlib.pyplot.show()\n",
    "  matplotlib.pyplot.close()\n",
    "\n",
    "  print(f\"\\n created and saved plot to '{output_file_name}'\")\n",
    "\n",
    "\n",
    "\n",
    "# # call the function here with the\n",
    "# fresh_model = EmbeddingNet(embedding_dim=128, backbone_name='resnet18')\n",
    "# fresh_model.to(device)\n",
    "# fresh_model.eval() # Set to evaluation mode\n",
    "\n",
    "# # Use the `get_all_embeddings` function and `eval_loader` to get embeddings from this new \"fresh\" model\n",
    "# base_embeddings, base_labels = get_all_embeddings(fresh_model, eval_loader, device)\n",
    "\n",
    "# print(\"plots before trainig\")\n",
    "# #fucntion call for the plots\n",
    "# plot_Umap_T_SNE(fresh_model,base_embeddings, base_labels, \"Visualization of Raw Image Features (from pre-trained ResNet18)\", 'cub_attributes_grid.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPY1P8b5jdBy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TripletCUBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom DataLoader to use CUB-200 for triplet loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(self.root_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        # Load metadata\n",
    "        self._load_metadata()\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        # Read images.txt: <image_id> <filepath>\n",
    "        images_df = pandas.read_csv(\n",
    "            os.path.join(self.root_dir, 'images.txt'),\n",
    "            sep=' ',\n",
    "            names=['img_id', 'filepath']\n",
    "        )\n",
    "\n",
    "        # Read image_class_labels.txt: <image_id> <class_id>\n",
    "        labels_df = pandas.read_csv(\n",
    "            os.path.join(self.root_dir, 'image_class_labels.txt'),\n",
    "            sep=' ',\n",
    "            names=['img_id', 'class_id']\n",
    "        )\n",
    "\n",
    "        # Read train_test_split.txt: <image_id> <is_train>\n",
    "        split_df = pandas.read_csv(\n",
    "            os.path.join(self.root_dir, 'train_test_split.txt'),\n",
    "            sep=' ',\n",
    "            names=['img_id', 'is_train']\n",
    "        )\n",
    "\n",
    "        # Merge dataframes\n",
    "        data_df = images_df.merge(labels_df, on='img_id').merge(split_df, on='img_id')\n",
    "\n",
    "        data_df['class_id'] = data_df['class_id'] - 1\n",
    "\n",
    "        # Filter by split (1 for train, 0 for test)\n",
    "        target_split = 1 if self.split == 'train' else 0\n",
    "        self.data_df = data_df[data_df['is_train'] == target_split].reset_index(drop=True)\n",
    "\n",
    "        # Create a list of all data points (filepath, class_id)\n",
    "        self.data_list = list(zip(self.data_df['filepath'], self.data_df['class_id']))\n",
    "\n",
    "        # Create a dictionary mapping class_id -> [list of indices in self.data_list]\n",
    "        self.class_to_indices = {}\n",
    "        for idx, (_, class_id) in enumerate(self.data_list):\n",
    "            if class_id not in self.class_to_indices:\n",
    "                self.class_to_indices[class_id] = []\n",
    "            self.class_to_indices[class_id].append(idx)\n",
    "\n",
    "        # Store a list of all unique class IDs\n",
    "        self.classes = list(self.class_to_indices.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    # def _load_image(self, filepath):\n",
    "    #     \"\"\"Helper to load an image from its relative path.\"\"\"\n",
    "    #     full_path = os.path.join(self.image_dir, filepath)\n",
    "    #     img = Image.open(full_path).convert('RGB')\n",
    "    #     return img\n",
    "\n",
    "    def _load_image(self, filepath):\n",
    "      \"\"\"Helper to load an image from its relative path.\"\"\"\n",
    "      full_path = os.path.join(self.image_dir, filepath)\n",
    "\n",
    "      try:\n",
    "          img = Image.open(full_path).convert('RGB')\n",
    "          return img\n",
    "      except OSError as e:\n",
    "          print(f\"!!!!!!!!!!!!!! ERROR LOADING IMAGE !!!!!!!!!!!!!!\")\n",
    "          print(f\"Failed to load image: {full_path}\")\n",
    "          print(f\"Error: {e}\")\n",
    "          raise e\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate a triplet!\n",
    "        \"\"\"\n",
    "\n",
    "        # get the ANCHOR\n",
    "        anchor_path, anchor_class = self.data_list[index]\n",
    "        anchor_img = self._load_image(anchor_path)\n",
    "\n",
    "        # get a POSITIVE image (same class, different image)\n",
    "        positive_indices = self.class_to_indices[anchor_class]\n",
    "\n",
    "        # Ensure we don't pick the same image as the anchor\n",
    "        positive_index = index\n",
    "        while positive_index == index and len(positive_indices) > 1:\n",
    "            positive_index = random.choice(positive_indices)\n",
    "\n",
    "        positive_path, _ = self.data_list[positive_index]\n",
    "        positive_img = self._load_image(positive_path)\n",
    "\n",
    "        # get NEGATIVE image (different class)\n",
    "        negative_class = anchor_class\n",
    "        while negative_class == anchor_class:\n",
    "            negative_class = random.choice(self.classes)\n",
    "\n",
    "        negative_indices = self.class_to_indices[negative_class]\n",
    "        negative_index = random.choice(negative_indices)\n",
    "\n",
    "        negative_path, _ = self.data_list[negative_index]\n",
    "        negative_img = self._load_image(negative_path)\n",
    "\n",
    "        # apply transforms\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            positive_img = self.transform(positive_img)\n",
    "            negative_img = self.transform(negative_img)\n",
    "\n",
    "        return anchor_img, positive_img, negative_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7zcwVAQlb3z",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS=20\n",
    "BATCH_SIZE=32\n",
    "LEARN_RATE=0.002\n",
    "NUM_WORKERS=0\n",
    "\n",
    "# train transforms (additional augmentations)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # % of images\n",
    "    transforms.RandomRotation(10), # rotates by up to x degrees\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# test transforms (no additional augmentatations)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# create datasets\n",
    "train_dataset = TripletCUBDataset(\n",
    "    root_dir=BASE_DIR,\n",
    "    split='train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "test_dataset = TripletCUBDataset(\n",
    "    root_dir=BASE_DIR,\n",
    "    split='test',\n",
    "    transform=test_transforms\n",
    ")\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# test loader\n",
    "print(\"\\nTesting the train_loader...\")\n",
    "try:\n",
    "    # Get one batch\n",
    "    anchor_batch, positive_batch, negative_batch = next(iter(train_loader))\n",
    "    print(f\"Anchor batch shape: {anchor_batch.shape}\")\n",
    "    print(f\"Positive batch shape: {positive_batch.shape}\")\n",
    "    print(f\"Negative batch shape: {negative_batch.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsPdGT7tmo4l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.optim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Helpers: batch cosine stats\n",
    "def batch_cosine_stats(a, p, n):\n",
    "    \"\"\"\n",
    "    a, p, n are L2-normalized torch embeddings of shape (B, D).\n",
    "    Returns numpy arrays sim_ap, sim_an (shape B,)\n",
    "    and batch_top1 (float in [0,1]).\n",
    "    \"\"\"\n",
    "    A = a.detach().cpu().numpy()\n",
    "    P = p.detach().cpu().numpy()\n",
    "    N = n.detach().cpu().numpy()\n",
    "\n",
    "    sim_ap = cosine_similarity(A, P).diagonal()\n",
    "    sim_an = cosine_similarity(A, N).diagonal()\n",
    "    top1 = (sim_ap > sim_an).mean()\n",
    "    return sim_ap, sim_an, float(top1)\n",
    "\n",
    "# Training\n",
    "def train(model, train_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # epoch accumulators\n",
    "    sum_ap = 0.0\n",
    "    sum_an = 0.0\n",
    "    sum_top1 = 0.0\n",
    "    n_examples = 0\n",
    "\n",
    "    for (anchor_img, positive_img, negative_img) in tqdm(train_loader, desc=\"Training\"):\n",
    "        # Move data to device\n",
    "        anchor_img  = anchor_img.to(device, non_blocking=True)\n",
    "        positive_img= positive_img.to(device, non_blocking=True)\n",
    "        negative_img= negative_img.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward to get embeddings\n",
    "        anchor_emb  = model(anchor_img)     # (B, D)\n",
    "        positive_emb= model(positive_img)   # (B, D)\n",
    "        negative_emb= model(negative_img)   # (B, D)\n",
    "\n",
    "        # L2-normalize so cosine â‰ˆ dot product\n",
    "        a = torch.nn.functional.normalize(anchor_emb,  p=2, dim=1)\n",
    "        p = torch.nn.functional.normalize(positive_emb,p=2, dim=1)\n",
    "        n = torch.nn.functional.normalize(negative_emb,p=2, dim=1)\n",
    "\n",
    "        # Batch cosine stats (before mining)\n",
    "        sim_ap, sim_an, batch_top1 = batch_cosine_stats(a, p, n)\n",
    "        B = a.size(0)\n",
    "        sum_ap   += float(sim_ap.sum())\n",
    "        sum_an   += float(sim_an.sum())\n",
    "        sum_top1 += batch_top1 * B\n",
    "        n_examples += B\n",
    "\n",
    "        loss = loss_fn(a, p, n)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += float(loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_mean_ap = sum_ap / n_examples\n",
    "    epoch_mean_an = sum_an / n_examples\n",
    "    epoch_top1    = sum_top1 / n_examples\n",
    "\n",
    "    return {\n",
    "        \"loss\": epoch_loss,\n",
    "        \"mean_ap\": epoch_mean_ap,\n",
    "        \"mean_an\": epoch_mean_an,\n",
    "        \"top1\": epoch_top1,\n",
    "        \"count\": n_examples,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWkseZREsEhF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ResNet18 training loop\n",
    "train_hist = []\n",
    "\n",
    "# Initialize the model\n",
    "embedding_dim = 512\n",
    "model_18 = EmbeddingNet(embedding_dim, backbone_name='resnet18')\n",
    "model_18.to(device)\n",
    "\n",
    "margin = 1.0\n",
    "loss_fn = torch.nn.TripletMarginLoss(margin=margin)\n",
    "optimizer = torch.optim.Adam(model_18.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "print(\"Starting ResNet18 training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    tr = train(model_18, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    train_hist.append(tr)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | {elapsed:.2f}s | \"\n",
    "        f\"Train Loss: {tr['loss']:.4f} | \"\n",
    "        f\"Train cos(AP): {tr['mean_ap']:.3f} | \"\n",
    "        f\"Train cos(AN): {tr['mean_an']:.3f} | \"\n",
    "        f\"Train Top1: {tr['top1']:.3f} \"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"ResNet18 training finished.\")\n",
    "\n",
    "# save model's weight and parameters\n",
    "MODEL_PATH_18 = 'resnet18_model.pth'\n",
    "torch.save(model_18.state_dict(), MODEL_PATH_18)\n",
    "\n",
    "print(f\"Model state_dict saved to {MODEL_PATH_18}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0-6n8FIsjEH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ResNet34 training loop\n",
    "train_hist = []\n",
    "\n",
    "# Initialize the model\n",
    "embedding_dim = 512\n",
    "model_34 = EmbeddingNet(embedding_dim, backbone_name='resnet34')\n",
    "model_34.to(device)\n",
    "\n",
    "margin = 1.0\n",
    "loss_fn = torch.nn.TripletMarginLoss(margin=margin)\n",
    "optimizer = torch.optim.Adam(model_34.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "print(\"Starting ResNet34 training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    tr = train(model_34, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    train_hist.append(tr)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | {elapsed:.2f}s | \"\n",
    "        f\"Train Loss: {tr['loss']:.4f} | \"\n",
    "        f\"Train cos(AP): {tr['mean_ap']:.3f} | \"\n",
    "        f\"Train cos(AN): {tr['mean_an']:.3f} | \"\n",
    "        f\"Train Top1: {tr['top1']:.3f} \"\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"ResNet34 training finished.\")\n",
    "\n",
    "# save model's weight and parameters\n",
    "MODEL_PATH_34 = 'resnet34_model.pth'\n",
    "torch.save(model_34.state_dict(), MODEL_PATH_34)\n",
    "\n",
    "print(f\"Model state_dict saved to {MODEL_PATH_34}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kQhA_u0wlXy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#A plot to show the loss function going down\n",
    "\n",
    "loss_hist = [d['loss'] for d in train_hist]\n",
    "\n",
    "# X axis based on actual length\n",
    "epochs_range = range(1, len(loss_hist) + 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, loss_hist, marker='o', label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot to file and display\n",
    "trainig_loss_graph = 'trainig_loss_graph.png'\n",
    "plt.savefig(trainig_loss_graph)\n",
    "\n",
    "print(f\"\\n created and saved plot to '{trainig_loss_graph}'\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPyH6ZbCInpK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test model\n",
    "def test(model, test_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Runs the model on the test set and returns a dictionary of metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # epoch accumulators\n",
    "    sum_ap = 0.0\n",
    "    sum_an = 0.0\n",
    "    sum_top1 = 0.0\n",
    "    n_examples = 0\n",
    "\n",
    "    # disable gradient calculations\n",
    "    with torch.no_grad():\n",
    "        for (anchor_img, positive_img, negative_img) in tqdm(test_loader, desc=\"Testing\"):\n",
    "            # move data to device\n",
    "            anchor_img  = anchor_img.to(device, non_blocking=True)\n",
    "            positive_img= positive_img.to(device, non_blocking=True)\n",
    "            negative_img= negative_img.to(device, non_blocking=True)\n",
    "\n",
    "            # get embeddings\n",
    "            anchor_emb  = model(anchor_img)\n",
    "            positive_emb= model(positive_img)\n",
    "            negative_emb= model(negative_img)\n",
    "\n",
    "            # L2-normalize already in EmbeddingNet (forward method)\n",
    "\n",
    "            # Batch cosine stats\n",
    "            sim_ap, sim_an, batch_top1 = batch_cosine_stats(anchor_emb, positive_emb, negative_emb)\n",
    "            B = anchor_emb.size(0)\n",
    "            sum_ap   += float(sim_ap.sum())\n",
    "            sum_an   += float(sim_an.sum())\n",
    "            sum_top1 += batch_top1 * B\n",
    "            n_examples += B\n",
    "\n",
    "            loss = loss_fn(anchor_emb, positive_emb, negative_emb)\n",
    "            running_loss += float(loss.item())\n",
    "\n",
    "    # final epoch metrics\n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    epoch_mean_ap = sum_ap / n_examples\n",
    "    epoch_mean_an = sum_an / n_examples\n",
    "    epoch_top1    = sum_top1 / n_examples\n",
    "\n",
    "    return {\n",
    "        \"loss\": epoch_loss,\n",
    "        \"mean_ap\": epoch_mean_ap,\n",
    "        \"mean_an\": epoch_mean_an,\n",
    "        \"top1\": epoch_top1,\n",
    "        \"count\": n_examples,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELsJUWirTOLF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# clear GPU memory\n",
    "# del model_18\n",
    "# del model_34\n",
    "# del optimizer\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"Freed GPU memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTVnikHEhvZk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test on ResNet18\n",
    "# using same margin and loss_fn from training run\n",
    "print(\"\\nTesting ResNet18 Model...\")\n",
    "\n",
    "# initialize and load model to device\n",
    "loaded_model_18 = EmbeddingNet(embedding_dim, backbone_name='resnet18')\n",
    "loaded_model_18.to(device)\n",
    "loaded_model_18.load_state_dict(torch.load(MODEL_PATH_18, map_location=device))\n",
    "\n",
    "test_stats_18 = test(model_18, test_loader, loss_fn, device)\n",
    "\n",
    "print(\"--- ResNet18 Test Results ---\")\n",
    "print(f\"  Loss: {test_stats_18['loss']:.4f}\")\n",
    "print(f\"  Top-1 Acc: {test_stats_18['top1'] * 100:.2f}%\")\n",
    "print(f\"  Avg. Pos Cosine: {test_stats_18['mean_ap']:.4f}\")\n",
    "print(f\"  Avg. Neg Cosine: {test_stats_18['mean_an']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQpeQXOxG7kL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test on ResNet34\n",
    "from PIL import ImageFile\n",
    "# allow truncated images to load (test dataset has truncated images)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "print(\"\\nTesting ResNet34 Model...\")\n",
    "\n",
    "# initialize and load model to device\n",
    "loaded_model_34 = EmbeddingNet(embedding_dim, backbone_name='resnet34')\n",
    "loaded_model_34.to(device)\n",
    "loaded_model_34.load_state_dict(torch.load(MODEL_PATH_34, map_location=device))\n",
    "\n",
    "test_stats_34 = test(model_34, test_loader, loss_fn, device)\n",
    "\n",
    "print(\"--- ResNet34 Test Results ---\")\n",
    "print(f\"  Loss: {test_stats_34['loss']:.4f}\")\n",
    "print(f\"  Top-1 Acc: {test_stats_34['top1'] * 100:.2f}%\")\n",
    "print(f\"  Avg. Pos Cosine: {test_stats_34['mean_ap']:.4f}\")\n",
    "print(f\"  Avg. Neg Cosine: {test_stats_34['mean_an']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fgeT99ZhvZl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# precision@k for ResNet18\n",
    "# get embeddings (using existing code before first UMAP / t-SNE plots)\n",
    "embeddings_18, labels_18 = get_all_embeddings(model_18, eval_loader, device)\n",
    "print(f\"got ResNet18 {embeddings_18.shape[0]} embeddings with {labels_18.shape[0]} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_zAXeT14Plm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# precision@k for ResNet34\n",
    "# get embeddings (using existing code before first UMAP / t-SNE plots)\n",
    "embeddings_34, labels_34 = get_all_embeddings(model_34, eval_loader, device)\n",
    "print(f\"got ResNet34 {embeddings_34.shape[0]} embeddings with {labels_34.shape[0]} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMGnnEOO4Yfm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_precision_at_k(embeddings, labels, k, device):\n",
    "    \"\"\"\n",
    "    Calculates Precision@k for a given set of embeddings and labels.\n",
    "    Assumes embeddings are L2-normalized (which your model does).\n",
    "    \"\"\"\n",
    "    num_embeddings = len(embeddings)\n",
    "\n",
    "    embeddings_gpu = embeddings.to(device)\n",
    "    labels_gpu = labels.to(device)\n",
    "\n",
    "    # cosine similarity; embeddings are L2-normalized, dot product = cosine similarity\n",
    "    similarity_matrix = torch.mm(embeddings_gpu, embeddings_gpu.T)\n",
    "\n",
    "    precisions = []\n",
    "\n",
    "    for i in tqdm(range(num_embeddings), desc=f\"Calculating P@{k}\"):\n",
    "        query_label = labels_gpu[i]\n",
    "\n",
    "        similarities = similarity_matrix[i]\n",
    "\n",
    "        # sort and get top k+1 indices (ignoring self-match)\n",
    "        # largest=True gets highest similarity\n",
    "        _, top_k_indices = torch.topk(similarities, k + 1, largest=True)\n",
    "\n",
    "        # remove self-match (highest similarity)\n",
    "        top_k_indices = top_k_indices[1:]\n",
    "\n",
    "        # labels of top k retrieved images\n",
    "        retrieved_labels = labels_gpu[top_k_indices]\n",
    "\n",
    "        # count matches with query label\n",
    "        num_correct = torch.sum(retrieved_labels == query_label).item()\n",
    "\n",
    "        # precision for this query\n",
    "        precision = num_correct / k\n",
    "        precisions.append(precision)\n",
    "\n",
    "    # average precision across all queries\n",
    "    return np.mean(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Zv3-bYFhvZl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ensure using device = cuda (NVIDIA or AMD)\n",
    "print(\"\\n--- ResNet18 Retrieval Results ---\")\n",
    "p_at_1 = calculate_precision_at_k(embeddings_18, labels_18, k=1, device=device)\n",
    "p_at_5 = calculate_precision_at_k(embeddings_18, labels_18, k=5, device=device)\n",
    "p_at_10 = calculate_precision_at_k(embeddings_18, labels_18, k=10, device=device)\n",
    "\n",
    "print(f\"  Precision@1:  {p_at_1 * 100:.2f}%\")\n",
    "print(f\"  Precision@5:  {p_at_5 * 100:.2f}%\")\n",
    "print(f\"  Precision@10: {p_at_10 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QodAAT75-6B",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ensure using device = cuda (NVIDIA or AMD)\n",
    "print(\"\\n--- ResNet34 Retrieval Results ---\")\n",
    "p_at_1 = calculate_precision_at_k(embeddings_34, labels_34, k=1, device=device)\n",
    "p_at_5 = calculate_precision_at_k(embeddings_34, labels_34, k=5, device=device)\n",
    "p_at_10 = calculate_precision_at_k(embeddings_34, labels_34, k=10, device=device)\n",
    "\n",
    "print(f\"  Precision@1:  {p_at_1 * 100:.2f}%\")\n",
    "print(f\"  Precision@5:  {p_at_5 * 100:.2f}%\")\n",
    "print(f\"  Precision@10: {p_at_10 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuxXdknGf54s",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#plot after training resnet18\n",
    "\n",
    "# Use the `get_all_embeddings` function and `eval_loader` to get embeddings from the model\n",
    "trained_resnet18_embeddings, trained_resnet18_labels = get_all_embeddings(model_18, eval_loader, device)\n",
    "\n",
    "print(\"plots before trainig\")\n",
    "#fucntion call for the plots\n",
    "plot_Umap_T_SNE(model_18, trained_resnet18_embeddings, trained_resnet18_labels, \"Visualization of images after training (ResNet18)\", 'trained_model_plot_resnet18.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uO71VRJtqbDT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#plot after training resnet34\n",
    "\n",
    "# Use the `get_all_embeddings` function and `eval_loader` to get embeddings from the model\n",
    "trained_resnet34_embeddings, trained_resnet34_labels = get_all_embeddings(model_34, eval_loader, device)\n",
    "\n",
    "print(\"plots before trainig\")\n",
    "#fucntion call for the plots\n",
    "plot_Umap_T_SNE(model_34, trained_resnet34_embeddings, trained_resnet34_labels, \"Visualization of images after training (ResNet34)\", 'trained_model_plot_resnet34.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
