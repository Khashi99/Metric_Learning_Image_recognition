{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e6ad75-ffc7-4dbc-b2f1-22ba7223287e",
   "metadata": {},
   "source": [
    "The bounding box, creates a crop of the image and takes aways the unimportant stuff (branches, leaves and ...) andjust leaves the birds.\n",
    "\n",
    "The cropped image then goes through the transform pipeline:\n",
    "- Resize ==> To make the images uniform!\n",
    "- Totensor ==> To convert the images into a pyTorch Tensor. meanig that you go from [H,W,C] to [C,H,W], C standig for the channel that has been normalized to be between 0 and 1 (critical for the calculations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeceba2-5de7-4ad9-a2e6-20378e708342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os\n",
    "import matplotlib.pyplot\n",
    "import numpy\n",
    "from PIL import Image\n",
    "\n",
    "# Image directory and files. Need to change the dir!\n",
    "CUB_ROOT_DIR = r'D:\\Concordia\\Fall_2025\\COMP_6831\\Midterm_project\\Fucking_around\\CUB_200_2011_reduced\\CUB_200_2011'\n",
    "IMAGE_DIR = os.path.join(CUB_ROOT_DIR, 'images')\n",
    "\n",
    "images_df = pandas.read_csv(os.path.join(CUB_ROOT_DIR, 'images.txt'), sep=' ', names=['img_id', 'filepath'])\n",
    "labels_df = pandas.read_csv(os.path.join(CUB_ROOT_DIR, 'image_class_labels.txt'), sep=' ', names=['img_id', 'class_id'])\n",
    "split_df = pandas.read_csv(os.path.join(CUB_ROOT_DIR, 'train_test_split.txt'), sep=' ', names=['img_id', 'is_train'])\n",
    "classes_df = pandas.read_csv(os.path.join(CUB_ROOT_DIR, 'classes.txt'), sep=' ', names=['class_id', 'class_name'])\n",
    "all_data_df = images_df.merge(labels_df, on='img_id').merge(split_df, on='img_id')\n",
    "\n",
    "# We'll need to chnage this. it was made for the 10 classes\n",
    "all_data_df['class_id'] = all_data_df['class_id'] - 1\n",
    "all_data_df = all_data_df[all_data_df['class_id'] < 10]\n",
    "\n",
    "# Show an image for each class\n",
    "print(\"Printing sample images\")\n",
    "fig, axes = matplotlib.pyplot.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    # Find the first image for this class\n",
    "    sample_row = all_data_df[all_data_df['class_id'] == i].iloc[0]\n",
    "    img_path = os.path.join(IMAGE_DIR, sample_row['filepath'])\n",
    "    class_name = classes_df[classes_df['class_id'] == (i + 1)]['class_name'].values[0].split('.')[-1]\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Class {i+1}: {class_name}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "matplotlib.pyplot.suptitle(\"Sample Image for Each of the Classes\", fontsize=16)\n",
    "matplotlib.pyplot.tight_layout()\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d44405-df0e-4698-9a99-9674dd09691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spurious attributes\n",
    "BASE_DIR = os.path.dirname(CUB_ROOT_DIR)\n",
    "\n",
    "# Load attribute names from the base directory\n",
    "try:\n",
    "    attributes_df = pandas.read_csv(\n",
    "        os.path.join(BASE_DIR, 'attributes.txt'),\n",
    "        sep=' ', \n",
    "        names=['attr_id', 'attr_name']\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    # Handle the case where the user's CUB_ROOT_DIR is the *actual* root\n",
    "    attributes_df = pandas.read_csv(\n",
    "        os.path.join(CUB_ROOT_DIR, '..', 'attributes.txt'), # Try relative path\n",
    "        sep=' ', \n",
    "        names=['attr_id', 'attr_name']\n",
    "    )\n",
    "\n",
    "\n",
    "# Load which images have which attributes (this path was correct)\n",
    "img_attr_df = pandas.read_csv(\n",
    "    os.path.join(CUB_ROOT_DIR, 'attributes', 'image_attribute_labels.txt'),\n",
    "    sep=' ', \n",
    "    names=['img_id', 'attr_id', 'is_present', 'certainty_id', 'time']\n",
    ")\n",
    "\n",
    "# Find the attribute for \"in water\"\n",
    "try:\n",
    "    water_attr = attributes_df[attributes_df['attr_name'].str.contains('water')].iloc[0]\n",
    "    water_attr_id = water_attr['attr_id']\n",
    "    print(f\"Found attribute: {water_attr['attr_name']} (ID: {water_attr_id})\\n\")\n",
    "\n",
    "    # Get all image_ids from our 10-class set\n",
    "    our_img_ids = set(all_data_df['img_id'])\n",
    "\n",
    "    # Filter the attribute labels for just our images and this attribute\n",
    "    water_labels = img_attr_df[\n",
    "        (img_attr_df['img_id'].isin(our_img_ids)) &\n",
    "        (img_attr_df['attr_id'] == water_attr_id) &\n",
    "        (img_attr_df['is_present'] == 1) # 1 = True, 0 = False\n",
    "    ]\n",
    "\n",
    "    # Merge with our main data to see class distribution\n",
    "    water_data = all_data_df.merge(water_labels, on='img_id', how='left')\n",
    "    water_data['is_in_water'] = water_data['is_present'].fillna(0).astype(int)\n",
    "\n",
    "    # Show an image for each group (e.g., in_water vs. not_in_water) ---\n",
    "    print(f\"Sample Images for Attribute '{water_attr['attr_name']}' \")\n",
    "    \n",
    "    # Check if we have samples for both groups\n",
    "    if 1 in water_data['is_in_water'].values and 0 in water_data['is_in_water'].values:\n",
    "        img_in_water = water_data[water_data['is_in_water'] == 1].iloc[0]\n",
    "        img_not_in_water = water_data[water_data['is_in_water'] == 0].iloc[0]\n",
    "\n",
    "        fig, axes = matplotlib.pyplot.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        img_path_1 = os.path.join(IMAGE_DIR, img_in_water['filepath'])\n",
    "        axes[0].imshow(Image.open(img_path_1))\n",
    "        axes[0].set_title(\"Group: 'is_in_water' = TRUE\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        img_path_2 = os.path.join(IMAGE_DIR, img_not_in_water['filepath'])\n",
    "        axes[1].imshow(Image.open(img_path_2))\n",
    "        axes[1].set_title(\"Group: 'is_in_water' = FALSE\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        matplotlib.pyplot.show()\n",
    "    else:\n",
    "        print(\"Could not find images for both 'in water' and 'not in water' groups.\")\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Could not find an attribute containing 'water' in your dataset.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find 'attributes.txt' at the expected location.\")\n",
    "    print(f\"Looked in: {os.path.join(BASE_DIR, 'attributes.txt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da3434-839e-4d5f-bbb8-913187f74ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional\n",
    "import torchvision.models\n",
    "\n",
    "class EmbeddingNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A network that uses a pretrained ResNet18 backbone to extract embeddings from images.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        \n",
    "        # Load the pretrained ResNet18\n",
    "        self.backbone = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # Get the number of features from the layer *before* the classifier\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # Replace the final classifier layer ('fc') with our new embedding layer\n",
    "        self.backbone.fc = torch.nn.Linear(num_features, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        \"\"\"\n",
    "        # 1. Pass image through the modified ResNet\n",
    "        embeddings = self.backbone(x)\n",
    "        \n",
    "        # 2. Normalize the embeddings (L2 normalization) to make the embedding vectors have a length of 1, standard practice for triplet loss.\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2b466-d86e-4093-8f19-56ec963d6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm #library for progress bars\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# Defining the Evaluation Dataset Class\n",
    "# This class returns (image, label) pairs\n",
    "class CUBEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that returns just one image and its label.\n",
    "    Used for generating embeddings for the entire test set.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, split='test', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(self.root_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        # Load metadata (same as before)\n",
    "        images_df = pandas.read_csv(os.path.join(self.root_dir, 'images.txt'), sep=' ', names=['img_id', 'filepath'])\n",
    "        labels_df = pandas.read_csv(os.path.join(self.root_dir, 'image_class_labels.txt'), sep=' ', names=['img_id', 'class_id'])\n",
    "        split_df = pandas.read_csv(os.path.join(self.root_dir, 'train_test_split.txt'), sep=' ', names=['img_id', 'is_train'])\n",
    "        \n",
    "        data_df = images_df.merge(labels_df, on='img_id').merge(split_df, on='img_id')\n",
    "        data_df['class_id'] = data_df['class_id'] - 1 # 0-indexed\n",
    "        \n",
    "        # Filter for first 10 classes. this should be removed for the real datase\n",
    "        data_df = data_df[data_df['class_id'] < 10].reset_index(drop=True)\n",
    "        \n",
    "        # Filter by split\n",
    "        target_split = 1 if self.split == 'train' else 0\n",
    "        self.data_df = data_df[data_df['is_train'] == target_split].reset_index(drop=True)\n",
    "        \n",
    "        self.data_list = list(zip(self.data_df['filepath'], self.data_df['class_id']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def _load_image(self, filepath):\n",
    "        full_path = os.path.join(self.image_dir, filepath)\n",
    "        img = Image.open(full_path).convert('RGB')\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the image and its label\n",
    "        img_path, label = self.data_list[index]\n",
    "        img = self._load_image(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, label\n",
    "\n",
    "# Defining the 'get_all_embeddings' function\n",
    "def get_all_embeddings(model, loader, device):\n",
    "    model.eval() # Set model to eval mode\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Getting Embeddings\"):\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            \n",
    "    # Concatenate all batches\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    return all_embeddings, all_labels\n",
    "\n",
    "# Defining the 'eval_loader' and 'data_transform'\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eval_dataset = CUBEmbeddingDataset(\n",
    "    root_dir=CUB_ROOT_DIR,\n",
    "    split='test',\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e72e10-ff9c-4d68-a5bd-69b0d1074f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install umap \n",
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d226116-ff7a-45f7-a4f3-1f31e2735bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP and T-SNE before the training \n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import transforms\n",
    "import umap\n",
    "\n",
    "# Get \"baseline\" embeddings\n",
    "print(\"--- Task 3: Baseline Visualization (Before Training) ---\")\n",
    "print(\"Initializing a fresh, pre-trained ResNet18...\")\n",
    "\n",
    "fresh_model = EmbeddingNet(embedding_dim=128) \n",
    "fresh_model.to(device)\n",
    "fresh_model.eval() # Set to evaluation mode\n",
    "\n",
    "# Use the `get_all_embeddings` function and `eval_loader` to get embeddings from this new \"fresh\" model.\n",
    "print(\"Generating baseline embeddings for the test set...\")\n",
    "base_embeddings, base_labels = get_all_embeddings(fresh_model, eval_loader, device)\n",
    "\n",
    "print(f\"Got {base_embeddings.shape[0]} baseline embeddings.\")\n",
    "\n",
    "\n",
    "# Run t-SNE \n",
    "tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n",
    "tsne_embeddings = tsne.fit_transform(base_embeddings)\n",
    "print(\"t-SNE finished.\")\n",
    "\n",
    "# Run UMAP \n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_embeddings = umap_reducer.fit_transform(base_embeddings)\n",
    "print(\"UMAP finished.\")\n",
    "\n",
    "# Plot both\n",
    "fig, axes = matplotlib.pyplot.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# t-SNE Plot\n",
    "axes[0].set_title('t-SNE of Baseline Embeddings (Before Training)')\n",
    "for i in range(10): # We have 10 classes\n",
    "    indices = (base_labels == i)\n",
    "    axes[0].scatter(\n",
    "        tsne_embeddings[indices, 0], \n",
    "        tsne_embeddings[indices, 1], \n",
    "        alpha=0.6,\n",
    "        label=f'Class {i+1}'\n",
    "    )\n",
    "axes[0].set_xlabel('t-SNE Component 1')\n",
    "axes[0].set_ylabel('t-SNE Component 2')\n",
    "axes[0].legend()\n",
    "\n",
    "# UMAP Plot\n",
    "axes[1].set_title('UMAP of Baseline Embeddings (Before Training)')\n",
    "for i in range(10): # We have 10 classes\n",
    "    indices = (base_labels == i)\n",
    "    axes[1].scatter(\n",
    "        umap_embeddings[indices, 0], \n",
    "        umap_embeddings[indices, 1], \n",
    "        alpha=0.6,\n",
    "        label=f'Class {i+1}'\n",
    "    )\n",
    "axes[1].set_xlabel('UMAP Component 1')\n",
    "axes[1].set_ylabel('UMAP Component 2')\n",
    "axes[1].legend()\n",
    "\n",
    "matplotlib.pyplot.suptitle(\"Visualization of Raw Image Features (from pre-trained ResNet18)\", fontsize=16)\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdecb68-08c5-4868-98ab-1ec44e961b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class TripletCUBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading CUB-200 for triplet loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(self.root_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        # Load metadata\n",
    "        self._load_metadata()\n",
    "        \n",
    "    def _load_metadata(self):\n",
    "        # Read images.txt: <image_id> <filepath>\n",
    "        images_df = pandas.read_csv(\n",
    "            os.path.join(self.root_dir, 'images.txt'),\n",
    "            sep=' ',\n",
    "            names=['img_id', 'filepath']\n",
    "        )\n",
    "        \n",
    "        # Read image_class_labels.txt: <image_id> <class_id>\n",
    "        labels_df = pandas.read_csv(\n",
    "            os.path.join(self.root_dir, 'image_class_labels.txt'),\n",
    "            sep=' ',\n",
    "            names=['img_id', 'class_id']\n",
    "        )\n",
    "        \n",
    "        # Read train_test_split.txt: <image_id> <is_train>\n",
    "        split_df = pandas.read_csv(\n",
    "            os.path.join(self.root_dir, 'train_test_split.txt'),\n",
    "            sep=' ',\n",
    "            names=['img_id', 'is_train']\n",
    "        )\n",
    "        \n",
    "        # Merge dataframes\n",
    "        data_df = images_df.merge(labels_df, on='img_id').merge(split_df, on='img_id')\n",
    "        \n",
    "        data_df['class_id'] = data_df['class_id'] - 1\n",
    "        \n",
    "        # Filter by split (1 for train, 0 for test)\n",
    "        target_split = 1 if self.split == 'train' else 0\n",
    "        self.data_df = data_df[data_df['is_train'] == target_split].reset_index(drop=True)\n",
    "\n",
    "        # Create a list of all data points (filepath, class_id)\n",
    "        self.data_list = list(zip(self.data_df['filepath'], self.data_df['class_id']))\n",
    "        \n",
    "        # Create a dictionary mapping class_id -> [list of indices in self.data_list]\n",
    "        self.class_to_indices = {}\n",
    "        for idx, (_, class_id) in enumerate(self.data_list):\n",
    "            if class_id not in self.class_to_indices:\n",
    "                self.class_to_indices[class_id] = []\n",
    "            self.class_to_indices[class_id].append(idx)\n",
    "            \n",
    "        # Store a list of all unique class IDs\n",
    "        self.classes = list(self.class_to_indices.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def _load_image(self, filepath):\n",
    "        \"\"\"Helper to load an image from its relative path.\"\"\"\n",
    "        full_path = os.path.join(self.image_dir, filepath)\n",
    "        img = Image.open(full_path).convert('RGB')\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate a triplet!\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the ANCHOR\n",
    "        anchor_path, anchor_class = self.data_list[index]\n",
    "        anchor_img = self._load_image(anchor_path)\n",
    "        \n",
    "        # Get a POSITIVE image (same class, different image) \n",
    "        positive_indices = self.class_to_indices[anchor_class]\n",
    "        \n",
    "        # Ensure we don't pick the same image as the anchor\n",
    "        positive_index = index\n",
    "        while positive_index == index and len(positive_indices) > 1:\n",
    "            positive_index = random.choice(positive_indices)\n",
    "            \n",
    "        positive_path, _ = self.data_list[positive_index]\n",
    "        positive_img = self._load_image(positive_path)\n",
    "        \n",
    "        # Get a NEGATIVE image (different class) \n",
    "        negative_class = anchor_class\n",
    "        while negative_class == anchor_class:\n",
    "            negative_class = random.choice(self.classes)\n",
    "            \n",
    "        negative_indices = self.class_to_indices[negative_class]\n",
    "        negative_index = random.choice(negative_indices)\n",
    "        \n",
    "        negative_path, _ = self.data_list[negative_index]\n",
    "        negative_img = self._load_image(negative_path)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            positive_img = self.transform(positive_img)\n",
    "            negative_img = self.transform(negative_img)\n",
    "            \n",
    "        return anchor_img, positive_img, negative_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887a6d2-74a2-4ff4-b650-766fc4ae1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Directory path\n",
    "CUB_ROOT_DIR = 'D:/Concordia/Fall_2025/COMP_6831/Midterm_project/Fucking_around/CUB_200_2011_reduced/CUB_200_2011' \n",
    "\n",
    "# Create the Dataset\n",
    "train_dataset = TripletCUBDataset(\n",
    "    root_dir=CUB_ROOT_DIR,\n",
    "    split='train',\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "test_dataset = TripletCUBDataset(\n",
    "    root_dir=CUB_ROOT_DIR,\n",
    "    split='test',\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Test the loader\n",
    "print(\"\\nTesting the train_loader...\")\n",
    "try:\n",
    "    # Get one batch\n",
    "    anchor_batch, positive_batch, negative_batch = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"Anchor batch shape: {anchor_batch.shape}\")\n",
    "    print(f\"Positive batch shape: {positive_batch.shape}\")\n",
    "    print(f\"Negative batch shape: {negative_batch.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please check your CUB_ROOT_DIR path and dataset integrity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f63960-3e80-4fad-bfdb-3467e95eb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional\n",
    "import torchvision.models\n",
    "\n",
    "# Initialize the model\n",
    "embedding_dim = 128\n",
    "model = EmbeddingNet(embedding_dim)\n",
    "\n",
    "# Check if a GPU is available and move the model to it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(model)\n",
    "\n",
    "# Test the model with one batch from your loader\n",
    "try:\n",
    "    anchor_batch, positive_batch, negative_batch = next(iter(train_loader))\n",
    "    \n",
    "    # Move batch to the same device as the model\n",
    "    anchor_batch = anchor_batch.to(device)\n",
    "    \n",
    "    # Perform a forward pass\n",
    "    model.eval() # Set model to evaluation mode for testing\n",
    "    with torch.no_grad(): # Don't calculate gradients\n",
    "        embeddings = model(anchor_batch)\n",
    "        \n",
    "    print(f\"\\nSuccessfully passed one batch through the model.\")\n",
    "    print(f\"Input batch shape: {anchor_batch.shape}\")\n",
    "    print(f\"Output embedding shape: {embeddings.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError testing model with data batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524000a-2e18-4f53-9f87-72af4300ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "import time\n",
    "\n",
    "#Setup Optimizer and Loss Function\n",
    "\n",
    "margin = 1.0\n",
    "loss_fn = torch.nn.TripletMarginLoss(margin=margin)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Training Function\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Handles the training logic for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for (anchor_img, positive_img, negative_img) in tqdm(train_loader, desc=\"Training\"):\n",
    "        # Move data to the device\n",
    "        anchor_img = anchor_img.to(device)\n",
    "        positive_img = positive_img.to(device)\n",
    "        negative_img = negative_img.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get embeddings\n",
    "        anchor_emb = model(anchor_img)\n",
    "        positive_emb = model(positive_img)\n",
    "        negative_emb = model(negative_img)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Validation Function\n",
    "\n",
    "def validate_one_epoch(model, test_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Handles the validation logic for one epoch.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # No gradients needed for validation\n",
    "        for (anchor_img, positive_img, negative_img) in tqdm(test_loader, desc=\"Validating\"):\n",
    "            # Move data to the device\n",
    "            anchor_img = anchor_img.to(device)\n",
    "            positive_img = positive_img.to(device)\n",
    "            negative_img = negative_img.to(device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            anchor_emb = model(anchor_img)\n",
    "            positive_emb = model(positive_img)\n",
    "            negative_emb = model(negative_img)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(anchor_emb, positive_emb, negative_emb)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    return running_loss / len(test_loader)\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate_one_epoch(model, test_loader, loss_fn, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Time: {end_time - start_time:.2f}s - \"\n",
    "          f\"Train Loss: {train_loss:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbb928-a981-4a27-bdae-70b7be45c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_losses, 'b-o', label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses, 'r-o', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533613be-4c93-47bc-9274-b6386311c6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
